{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "/home/yxjiang/source/ml_playground\n"
    }
   ],
   "source": [
    "# env setup\n",
    "import os\n",
    "import sys\n",
    "\n",
    "os.chdir(\"/home/yxjiang/source/ml_playground\")\n",
    "\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Destination folder [/tmp/data] exists.\nTarget file [aclImdb_v1.tar.gz] exists, skip downloading.\nStart to extract [/tmp/data/aclImdb_v1.tar.gz] to [/tmp/data]...\nFile extracted\nProcessing vocabulary from [/tmp/data/aclImdb].\nThere size of vocabulary is : 89527\n"
    }
   ],
   "source": [
    "# data downloading\n",
    "from util import data_util\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# dataset_url=\"https://s3.amazonaws.com/fast-ai-nlp/dbpedia_csv.tgz\"\n",
    "dataset_url=\"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "\n",
    "dest_dir = \"/tmp/data\"\n",
    "dataset_folder_path = os.path.join(dest_dir, \"aclImdb\")\n",
    "data_util.download_data(url=dataset_url, dest_dir=dest_dir)\n",
    "\n",
    "# generate word to id mapping\n",
    "word_to_id = data_util.get_vocabulary(folder_path=dataset_folder_path, file_suffix=\"vocab\")\n",
    "print(\"There size of vocabulary is :\", len(word_to_id))\n",
    "\n",
    "# generate class id to name mapping\n",
    "# class_to_name = defaultdict(str)\n",
    "# with open(os.path.join(dataset_folder_path, \"classes.txt\"), \"r\") as f:\n",
    "#     for i, class_name in enumerate(f):\n",
    "#         class_to_name[i] = class_name.strip()\n",
    "# print(\"There class mapping:\", class_to_name.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configs\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.num_classes = 2\n",
    "        self.sentence_max_length = 30\n",
    "        self.word_embedding_length = 32\n",
    "        self.activation = F.relu\n",
    "        self.criteria = nn.CrossEntropyLoss\n",
    "        self.optimizer = optim.Adam\n",
    "        self.lr = 0.001\n",
    "        self.epochs = 5000\n",
    "        self.batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform, dataset and dataloader\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "class CutOrPadTransform:\n",
    "    \"\"\"\n",
    "    Shape all sentences to the equal length.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "    \n",
    "    def __call__(self, input):\n",
    "        if len(input['words']) >= config.sentence_max_length:\n",
    "            input['words'] = input['words'][:config.sentence_max_length]\n",
    "        else:\n",
    "            input['words'].extend([' '] * (config.sentence_max_length - len(input['words'])))\n",
    "        return input\n",
    "\n",
    "\n",
    "class WordsToIdsTransform:\n",
    "    \"\"\"\n",
    "    Convert the list of words to embeddings.\n",
    "    \"\"\"\n",
    "    def __init__(self, config, word_to_id):\n",
    "        self.config = config\n",
    "        self.word_to_id = word_to_id\n",
    "    \n",
    "    def __call__(self, input):\n",
    "        input['word_ids'] = torch.tensor([word_to_id[w] for w in input['words']], dtype=torch.long)\n",
    "        # del input['words']\n",
    "        return input\n",
    "\n",
    "\n",
    "class MovieReviewDataset(Dataset):\n",
    "    def __init__(self, config, pos_data_folder, neg_data_folder, word_to_id, transform):\n",
    "        self.config = config\n",
    "        self.word_to_id = word_to_id\n",
    "        self.data = []\n",
    "        # read all data into memory\n",
    "        for filename in os.listdir(pos_data_folder):\n",
    "            if filename.endswith(\".txt\"):\n",
    "                with open(os.path.join(pos_data_folder, filename), \"r\") as f:\n",
    "                    self.data.append((f.readline(), 1))\n",
    "\n",
    "        for filename in os.listdir(pos_data_folder):\n",
    "            if filename.endswith(\".txt\"):\n",
    "                with open(os.path.join(pos_data_folder, filename), \"r\") as f:\n",
    "                    self.data.append((f.readline(), 0))\n",
    "\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        words = [w.strip() for w in self.data[idx][0].strip().split(\" \")]\n",
    "        label = self.data[idx][1]\n",
    "        input = self.transform({'words': words, 'label': label})\n",
    "        return input['words'], input['word_ids'], input['label']\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, config, vocabulary_size):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embed = nn.Embedding(vocabulary_size, config.word_embedding_length)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(1, 1, kernel_size=(3, config.word_embedding_length))\n",
    "        self.conv4 = nn.Conv2d(1, 1, kernel_size=(4, config.word_embedding_length))\n",
    "        self.conv5 = nn.Conv2d(1, 1, kernel_size=(5, config.word_embedding_length))\n",
    "        self.conv6 = nn.Conv2d(1, 1, kernel_size=(6, config.word_embedding_length))\n",
    "        self.conv7 = nn.Conv2d(1, 1, kernel_size=(7, config.word_embedding_length))\n",
    "\n",
    "        self.max_over_time_pool3 = nn.MaxPool2d((config.sentence_max_length - 2, 1))\n",
    "        self.max_over_time_pool4 = nn.MaxPool2d((config.sentence_max_length - 3, 1))\n",
    "        self.max_over_time_pool5 = nn.MaxPool2d((config.sentence_max_length - 4, 1))\n",
    "        self.max_over_time_pool6 = nn.MaxPool2d((config.sentence_max_length - 5, 1))\n",
    "        self.max_over_time_pool7 = nn.MaxPool2d((config.sentence_max_length - 6, 1))\n",
    "\n",
    "        self.fc = nn.Linear(5, config.num_classes)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch = x.shape[0]\n",
    "        x = torch.unsqueeze(self.embed(x), 1)  # [NCHW]\n",
    "        c = self.conv3(x)\n",
    "        # convs\n",
    "        x1 = self.config.activation(self.conv3(x))\n",
    "        x2 = self.config.activation(self.conv4(x))\n",
    "        x3 = self.config.activation(self.conv5(x))\n",
    "        x4 = self.config.activation(self.conv6(x))\n",
    "        x5 = self.config.activation(self.conv7(x))\n",
    "\n",
    "        # max over time pooling\n",
    "        x1 = self.max_over_time_pool3(x1)\n",
    "        x2 = self.max_over_time_pool4(x2)\n",
    "        x3 = self.max_over_time_pool5(x3)\n",
    "        x4 = self.max_over_time_pool6(x4)\n",
    "        x5 = self.max_over_time_pool7(x5)\n",
    "\n",
    "        x = torch.cat((x1, x2, x3, x4, x5), dim=-1)\n",
    "        x = self.fc(x)\n",
    "        x = x.view(batch, -1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer\n",
    "def train(model, config, train_dataloader, device, check_interval=1000):\n",
    "    criteria = config.criteria()\n",
    "    optimizer = config.optimizer(model.parameters(), config.lr)\n",
    "    start = time.time()\n",
    "    counts = 0\n",
    "    for epoch in range(config.epochs):\n",
    "        for i, (words, word_ids, labels) in enumerate(train_dataloader):\n",
    "            counts += labels.shape[0]\n",
    "            output = model(word_ids.to(device))\n",
    "            loss = criteria(output, labels.to(device))\n",
    "            optimizer.zero_grad()\n",
    "            optimizer.step()\n",
    "            if ((epoch + 1) * i) % check_interval == 0:\n",
    "                print(\"[%d seconds](epoch: %d/%d)[%d samples] loss: %.3f.\" % (time.time() - start, epoch + 1, config.epochs, counts, loss.mean().item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "loss: 0.690.\n[58 seconds](epoch: 49/5000)[1200128 samples] loss: 0.696.\n[59 seconds](epoch: 50/5000)[1225128 samples] loss: 0.699.\n[59 seconds](epoch: 50/5000)[1227688 samples] loss: 0.689.\n[59 seconds](epoch: 50/5000)[1230248 samples] loss: 0.696.\n[59 seconds](epoch: 50/5000)[1232808 samples] loss: 0.701.\n[59 seconds](epoch: 50/5000)[1235368 samples] loss: 0.695.\n[59 seconds](epoch: 50/5000)[1237928 samples] loss: 0.697.\n[60 seconds](epoch: 50/5000)[1240488 samples] loss: 0.688.\n[60 seconds](epoch: 50/5000)[1243048 samples] loss: 0.690.\n[60 seconds](epoch: 50/5000)[1245608 samples] loss: 0.695.\n[60 seconds](epoch: 50/5000)[1248168 samples] loss: 0.695.\n[60 seconds](epoch: 51/5000)[1250128 samples] loss: 0.702.\n[61 seconds](epoch: 52/5000)[1275128 samples] loss: 0.688.\n[63 seconds](epoch: 53/5000)[1300128 samples] loss: 0.704.\n[64 seconds](epoch: 54/5000)[1325128 samples] loss: 0.698.\n[65 seconds](epoch: 55/5000)[1350128 samples] loss: 0.695.\n[66 seconds](epoch: 56/5000)[1375128 samples] loss: 0.697.\n[67 seconds](epoch: 56/5000)[1391128 samples] loss: 0.689.\n[67 seconds](epoch: 57/5000)[1400128 samples] loss: 0.702.\n[69 seconds](epoch: 58/5000)[1425128 samples] loss: 0.690.\n[70 seconds](epoch: 59/5000)[1450128 samples] loss: 0.692.\n[71 seconds](epoch: 60/5000)[1475128 samples] loss: 0.687.\n[71 seconds](epoch: 60/5000)[1481528 samples] loss: 0.702.\n[72 seconds](epoch: 60/5000)[1487928 samples] loss: 0.687.\n[72 seconds](epoch: 60/5000)[1494328 samples] loss: 0.688.\n[72 seconds](epoch: 61/5000)[1500128 samples] loss: 0.695.\n[73 seconds](epoch: 62/5000)[1525128 samples] loss: 0.695.\n[74 seconds](epoch: 63/5000)[1550128 samples] loss: 0.698.\n[76 seconds](epoch: 64/5000)[1575128 samples] loss: 0.705.\n[77 seconds](epoch: 64/5000)[1591128 samples] loss: 0.690.\n[77 seconds](epoch: 65/5000)[1600128 samples] loss: 0.706.\n[78 seconds](epoch: 66/5000)[1625128 samples] loss: 0.695.\n[79 seconds](epoch: 67/5000)[1650128 samples] loss: 0.687.\n[81 seconds](epoch: 68/5000)[1675128 samples] loss: 0.695.\n[82 seconds](epoch: 69/5000)[1700128 samples] loss: 0.694.\n[83 seconds](epoch: 70/5000)[1725128 samples] loss: 0.690.\n[84 seconds](epoch: 70/5000)[1737928 samples] loss: 0.696.\n[84 seconds](epoch: 71/5000)[1750128 samples] loss: 0.692.\n[85 seconds](epoch: 72/5000)[1775128 samples] loss: 0.701.\n[86 seconds](epoch: 72/5000)[1791128 samples] loss: 0.701.\n[87 seconds](epoch: 73/5000)[1800128 samples] loss: 0.700.\n[88 seconds](epoch: 74/5000)[1825128 samples] loss: 0.691.\n[89 seconds](epoch: 75/5000)[1850128 samples] loss: 0.697.\n[89 seconds](epoch: 75/5000)[1855248 samples] loss: 0.691.\n[90 seconds](epoch: 75/5000)[1860368 samples] loss: 0.697.\n[90 seconds](epoch: 75/5000)[1865488 samples] loss: 0.694.\n[90 seconds](epoch: 75/5000)[1870608 samples] loss: 0.699.\n[90 seconds](epoch: 76/5000)[1875128 samples] loss: 0.693.\n[92 seconds](epoch: 77/5000)[1900128 samples] loss: 0.691.\n[93 seconds](epoch: 78/5000)[1925128 samples] loss: 0.689.\n[94 seconds](epoch: 79/5000)[1950128 samples] loss: 0.703.\n[95 seconds](epoch: 80/5000)[1975128 samples] loss: 0.692.\n[95 seconds](epoch: 80/5000)[1978328 samples] loss: 0.706.\n[96 seconds](epoch: 80/5000)[1981528 samples] loss: 0.697.\n[96 seconds](epoch: 80/5000)[1984728 samples] loss: 0.692.\n[96 seconds](epoch: 80/5000)[1987928 samples] loss: 0.697.\n[96 seconds](epoch: 80/5000)[1991128 samples] loss: 0.693.\n[96 seconds](epoch: 80/5000)[1994328 samples] loss: 0.701.\n[96 seconds](epoch: 80/5000)[1997528 samples] loss: 0.693.\n[96 seconds](epoch: 81/5000)[2000128 samples] loss: 0.690.\n[98 seconds](epoch: 82/5000)[2025128 samples] loss: 0.689.\n[99 seconds](epoch: 83/5000)[2050128 samples] loss: 0.698.\n[100 seconds](epoch: 84/5000)[2075128 samples] loss: 0.705.\n[101 seconds](epoch: 85/5000)[2100128 samples] loss: 0.696.\n[102 seconds](epoch: 86/5000)[2125128 samples] loss: 0.694.\n[104 seconds](epoch: 87/5000)[2150128 samples] loss: 0.695.\n[105 seconds](epoch: 88/5000)[2175128 samples] loss: 0.695.\n[106 seconds](epoch: 88/5000)[2191128 samples] loss: 0.694.\n[106 seconds](epoch: 89/5000)[2200128 samples] loss: 0.691.\n[107 seconds](epoch: 90/5000)[2225128 samples] loss: 0.699.\n[108 seconds](epoch: 90/5000)[2237928 samples] loss: 0.695.\n[108 seconds](epoch: 91/5000)[2250128 samples] loss: 0.691.\n[110 seconds](epoch: 92/5000)[2275128 samples] loss: 0.703.\n[111 seconds](epoch: 93/5000)[2300128 samples] loss: 0.700.\n[112 seconds](epoch: 94/5000)[2325128 samples] loss: 0.688.\n[113 seconds](epoch: 95/5000)[2350128 samples] loss: 0.692.\n[115 seconds](epoch: 96/5000)[2375128 samples] loss: 0.694.\n[115 seconds](epoch: 96/5000)[2391128 samples] loss: 0.701.\n[116 seconds](epoch: 97/5000)[2400128 samples] loss: 0.693.\n[117 seconds](epoch: 98/5000)[2425128 samples] loss: 0.691.\n[118 seconds](epoch: 99/5000)[2450128 samples] loss: 0.704.\n[119 seconds](epoch: 100/5000)[2475128 samples] loss: 0.702.\n[119 seconds](epoch: 100/5000)[2476408 samples] loss: 0.695.\n[120 seconds](epoch: 100/5000)[2477688 samples] loss: 0.693.\n[120 seconds](epoch: 100/5000)[2478968 samples] loss: 0.700.\n[120 seconds](epoch: 100/5000)[2480248 samples] loss: 0.703.\n[120 seconds](epoch: 100/5000)[2481528 samples] loss: 0.691.\n[120 seconds](epoch: 100/5000)[2482808 samples] loss: 0.704.\n[120 seconds](epoch: 100/5000)[2484088 samples] loss: 0.696.\n[120 seconds](epoch: 100/5000)[2485368 samples] loss: 0.678.\n[120 seconds](epoch: 100/5000)[2486648 samples] loss: 0.694.\n[120 seconds](epoch: 100/5000)[2487928 samples] loss: 0.695.\n[120 seconds](epoch: 100/5000)[2489208 samples] loss: 0.698.\n[120 seconds](epoch: 100/5000)[2490488 samples] loss: 0.694.\n[120 seconds](epoch: 100/5000)[2491768 samples] loss: 0.686.\n[120 seconds](epoch: 100/5000)[2493048 samples] loss: 0.687.\n[120 seconds](epoch: 100/5000)[2494328 samples] loss: 0.702.\n[120 seconds](epoch: 100/5000)[2495608 samples] loss: 0.694.\n[121 seconds](epoch: 100/5000)[2496888 samples] loss: 0.692.\n[121 seconds](epoch: 100/5000)[2498168 samples] loss: 0.694.\n[121 seconds](epoch: 100/5000)[2499448 samples] loss: 0.686.\n[121 seconds](epoch: 101/5000)[2500128 samples] loss: 0.691.\n[122 seconds](epoch: 102/5000)[2525128 samples] loss: 0.688.\n[123 seconds](epoch: 103/5000)[2550128 samples] loss: 0.695.\n[124 seconds](epoch: 104/5000)[2575128 samples] loss: 0.693.\n[125 seconds](epoch: 104/5000)[2591128 samples] loss: 0.698.\n[126 seconds](epoch: 105/5000)[2600128 samples] loss: 0.696.\n[127 seconds](epoch: 106/5000)[2625128 samples] loss: 0.698.\n[128 seconds](epoch: 107/5000)[2650128 samples] loss: 0.696.\n[129 seconds](epoch: 108/5000)[2675128 samples] loss: 0.686.\n[130 seconds](epoch: 109/5000)[2700128 samples] loss: 0.706.\n[132 seconds](epoch: 110/5000)[2725128 samples] loss: 0.699.\n[132 seconds](epoch: 110/5000)[2737928 samples] loss: 0.697.\n[133 seconds](epoch: 111/5000)[2750128 samples] loss: 0.691.\n[134 seconds](epoch: 112/5000)[2775128 samples] loss: 0.699.\n[135 seconds](epoch: 112/5000)[2791128 samples] loss: 0.695.\n[135 seconds](epoch: 113/5000)[2800128 samples] loss: 0.699.\n[136 seconds](epoch: 114/5000)[2825128 samples] loss: 0.703.\n[138 seconds](epoch: 115/5000)[2850128 samples] loss: 0.703.\n[139 seconds](epoch: 116/5000)[2875128 samples] loss: 0.700.\n[140 seconds](epoch: 117/5000)[2900128 samples] loss: 0.695.\n[141 seconds](epoch: 118/5000)[2925128 samples] loss: 0.686.\n[142 seconds](epoch: 119/5000)[2950128 samples] loss: 0.698.\n[144 seconds](epoch: 120/5000)[2975128 samples] loss: 0.695.\n[144 seconds](epoch: 120/5000)[2978328 samples] loss: 0.691.\n[144 seconds](epoch: 120/5000)[2981528 samples] loss: 0.685.\n[144 seconds](epoch: 120/5000)[2984728 samples] loss: 0.693.\n[144 seconds](epoch: 120/5000)[2987928 samples] loss: 0.689.\n[144 seconds](epoch: 120/5000)[2991128 samples] loss: 0.688.\n[145 seconds](epoch: 120/5000)[2994328 samples] loss: 0.697.\n[145 seconds](epoch: 120/5000)[2997528 samples] loss: 0.701.\n[145 seconds](epoch: 121/5000)[3000128 samples] loss: 0.701.\n[146 seconds](epoch: 122/5000)[3025128 samples] loss: 0.706.\n[147 seconds](epoch: 123/5000)[3050128 samples] loss: 0.704.\n[148 seconds](epoch: 124/5000)[3075128 samples] loss: 0.685.\n[150 seconds](epoch: 125/5000)[3100128 samples] loss: 0.692.\n[150 seconds](epoch: 125/5000)[3101152 samples] loss: 0.684.\n[150 seconds](epoch: 125/5000)[3102176 samples] loss: 0.700.\n[150 seconds](epoch: 125/5000)[3103200 samples] loss: 0.694.\n[150 seconds](epoch: 125/5000)[3104224 samples] loss: 0.696.\n[150 seconds](epoch: 125/5000)[3105248 samples] loss: 0.698.\n[150 seconds](epoch: 125/5000)[3106272 samples] loss: 0.695.\n[150 seconds](epoch: 125/5000)[3107296 samples] loss: 0.692.\n[150 seconds](epoch: 125/5000)[3108320 samples] loss: 0.698.\n[150 seconds](epoch: 125/5000)[3109344 samples] loss: 0.696.\n[150 seconds](epoch: 125/5000)[3110368 samples] loss: 0.687.\n[150 seconds](epoch: 125/5000)[3111392 samples] loss: 0.691.\n[150 seconds](epoch: 125/5000)[3112416 samples] loss: 0.711.\n[150 seconds](epoch: 125/5000)[3113440 samples] loss: 0.687.\n[150 seconds](epoch: 125/5000)[3114464 samples] loss: 0.698.\n[151 seconds](epoch: 125/5000)[3115488 samples] loss: 0.700.\n[151 seconds](epoch: 125/5000)[3116512 samples] loss: 0.696.\n[151 seconds](epoch: 125/5000)[3117536 samples] loss: 0.702.\n[151 seconds](epoch: 125/5000)[3118560 samples] loss: 0.694.\n[151 seconds](epoch: 125/5000)[3119584 samples] loss: 0.686.\n[151 seconds](epoch: 125/5000)[3120608 samples] loss: 0.694.\n[151 seconds](epoch: 125/5000)[3121632 samples] loss: 0.698.\n[151 seconds](epoch: 125/5000)[3122656 samples] loss: 0.694.\n[151 seconds](epoch: 125/5000)[3123680 samples] loss: 0.697.\n[151 seconds](epoch: 125/5000)[3124704 samples] loss: 0.685.\n[151 seconds](epoch: 126/5000)[3125128 samples] loss: 0.689.\n[152 seconds](epoch: 127/5000)[3150128 samples] loss: 0.689.\n[153 seconds](epoch: 128/5000)[3175128 samples] loss: 0.697.\n[154 seconds](epoch: 128/5000)[3191128 samples] loss: 0.690.\n[155 seconds](epoch: 129/5000)[3200128 samples] loss: 0.688.\n[156 seconds](epoch: 130/5000)[3225128 samples] loss: 0.703.\n[156 seconds](epoch: 130/5000)[3237928 samples] loss: 0.700.\n[157 seconds](epoch: 131/5000)[3250128 samples] loss: 0.696.\n[158 seconds](epoch: 132/5000)[3275128 samples] loss: 0.701.\n[159 seconds](epoch: 133/5000)[3300128 samples] loss: 0.687.\n[161 seconds](epoch: 134/5000)[3325128 samples] loss: 0.699.\n[162 seconds](epoch: 135/5000)[3350128 samples] loss: 0.693.\n[163 seconds](epoch: 136/5000)[3375128 samples] loss: 0.701.\n[164 seconds](epoch: 136/5000)[3391128 samples] loss: 0.686.\n[164 seconds](epoch: 137/5000)[3400128 samples] loss: 0.697.\n[165 seconds](epoch: 138/5000)[3425128 samples] loss: 0.701.\n[167 seconds](epoch: 139/5000)[3450128 samples] loss: 0.693.\n[168 seconds](epoch: 140/5000)[3475128 samples] loss: 0.692.\n[168 seconds](epoch: 140/5000)[3481528 samples] loss: 0.695.\n[169 seconds](epoch: 140/5000)[3487928 samples] loss: 0.693.\n[169 seconds](epoch: 140/5000)[3494328 samples] loss: 0.703.\n[169 seconds](epoch: 141/5000)[3500128 samples] loss: 0.697.\n[170 seconds](epoch: 142/5000)[3525128 samples] loss: 0.694.\n[171 seconds](epoch: 143/5000)[3550128 samples] loss: 0.703.\n[173 seconds](epoch: 144/5000)[3575128 samples] loss: 0.689.\n[174 seconds](epoch: 144/5000)[3591128 samples] loss: 0.699.\n[174 seconds](epoch: 145/5000)[3600128 samples] loss: 0.701.\n[175 seconds](epoch: 146/5000)[3625128 samples] loss: 0.701.\n[176 seconds](epoch: 147/5000)[3650128 samples] loss: 0.689.\n[178 seconds](epoch: 148/5000)[3675128 samples] loss: 0.691.\n[179 seconds](epoch: 149/5000)[3700128 samples] loss: 0.704.\n[180 seconds](epoch: 150/5000)[3725128 samples] loss: 0.697.\n[180 seconds](epoch: 150/5000)[3727688 samples] loss: 0.692.\n[180 seconds](epoch: 150/5000)[3730248 samples] loss: 0.699.\n[180 seconds](epoch: 150/5000)[3732808 samples] loss: 0.700.\n[180 seconds](epoch: 150/5000)[3735368 samples] loss: 0.702.\n[181 seconds](epoch: 150/5000)[3737928 samples] loss: 0.695.\n[181 seconds](epoch: 150/5000)[3740488 samples] loss: 0.712.\n[181 seconds](epoch: 150/5000)[3743048 samples] loss: 0.693.\n[181 seconds](epoch: 150/5000)[3745608 samples] loss: 0.696.\n[181 seconds](epoch: 150/5000)[3748168 samples] loss: 0.697.\n[181 seconds](epoch: 151/5000)[3750128 samples] loss: 0.698.\n[182 seconds](epoch: 152/5000)[3775128 samples] loss: 0.698.\n[183 seconds](epoch: 152/5000)[3791128 samples] loss: 0.694.\n[184 seconds](epoch: 153/5000)[3800128 samples] loss: 0.696.\n[185 seconds](epoch: 154/5000)[3825128 samples] loss: 0.696.\n[186 seconds](epoch: 155/5000)[3850128 samples] loss: 0.682.\n[187 seconds](epoch: 156/5000)[3875128 samples] loss: 0.704.\n[189 seconds](epoch: 157/5000)[3900128 samples] loss: 0.686.\n[190 seconds](epoch: 158/5000)[3925128 samples] loss: 0.700.\n[191 seconds](epoch: 159/5000)[3950128 samples] loss: 0.698.\n[192 seconds](epoch: 160/5000)[3975128 samples] loss: 0.697.\n[192 seconds](epoch: 160/5000)[3978328 samples] loss: 0.693.\n[192 seconds](epoch: 160/5000)[3981528 samples] loss: 0.696.\n[193 seconds](epoch: 160/5000)[3984728 samples] loss: 0.693.\n[193 seconds](epoch: 160/5000)[3987928 samples] loss: 0.691.\n[193 seconds](epoch: 160/5000)[3991128 samples] loss: 0.688.\n[193 seconds](epoch: 160/5000)[3994328 samples] loss: 0.697.\n[193 seconds](epoch: 160/5000)[3997528 samples] loss: 0.694.\n[193 seconds](epoch: 161/5000)[4000128 samples] loss: 0.694.\n[194 seconds](epoch: 162/5000)[4025128 samples] loss: 0.697.\n[196 seconds](epoch: 163/5000)[4050128 samples] loss: 0.704.\n[197 seconds](epoch: 164/5000)[4075128 samples] loss: 0.686.\n[198 seconds](epoch: 165/5000)[4100128 samples] loss: 0.698.\n[199 seconds](epoch: 166/5000)[4125128 samples] loss: 0.699.\n[200 seconds](epoch: 167/5000)[4150128 samples] loss: 0.691.\n[202 seconds](epoch: 168/5000)[4175128 samples] loss: 0.699.\n[203 seconds](epoch: 168/5000)[4191128 samples] loss: 0.688.\n[203 seconds](epoch: 169/5000)[4200128 samples] loss: 0.691.\n[204 seconds](epoch: 170/5000)[4225128 samples] loss: 0.698.\n[205 seconds](epoch: 170/5000)[4237928 samples] loss: 0.700.\n[205 seconds](epoch: 171/5000)[4250128 samples] loss: 0.690.\n[207 seconds](epoch: 172/5000)[4275128 samples] loss: 0.689.\n[208 seconds](epoch: 173/5000)[4300128 samples] loss: 0.684.\n[209 seconds](epoch: 174/5000)[4325128 samples] loss: 0.704.\n[210 seconds](epoch: 175/5000)[4350128 samples] loss: 0.693.\n[210 seconds](epoch: 175/5000)[4355248 samples] loss: 0.691.\n[211 seconds](epoch: 175/5000)[4360368 samples] loss: 0.696.\n[211 seconds](epoch: 175/5000)[4365488 samples] loss: 0.699.\n[211 seconds](epoch: 175/5000)[4370608 samples] loss: 0.692.\n[211 seconds](epoch: 176/5000)[4375128 samples] loss: 0.692.\n[212 seconds](epoch: 176/5000)[4391128 samples] loss: 0.685.\n[213 seconds](epoch: 177/5000)[4400128 samples] loss: 0.700.\n[214 seconds](epoch: 178/5000)[4425128 samples] loss: 0.700.\n[215 seconds](epoch: 179/5000)[4450128 samples] loss: 0.689.\n[216 seconds](epoch: 180/5000)[4475128 samples] loss: 0.689.\n[217 seconds](epoch: 180/5000)[4481528 samples] loss: 0.701.\n[217 seconds](epoch: 180/5000)[4487928 samples] loss: 0.690.\n[217 seconds](epoch: 180/5000)[4494328 samples] loss: 0.695.\n[218 seconds](epoch: 181/5000)[4500128 samples] loss: 0.697.\n[219 seconds](epoch: 182/5000)[4525128 samples] loss: 0.698.\n[220 seconds](epoch: 183/5000)[4550128 samples] loss: 0.706.\n[221 seconds](epoch: 184/5000)[4575128 samples] loss: 0.706.\n[222 seconds](epoch: 184/5000)[4591128 samples] loss: 0.705.\n[222 seconds](epoch: 185/5000)[4600128 samples] loss: 0.693.\n[224 seconds](epoch: 186/5000)[4625128 samples] loss: 0.694.\n[225 seconds](epoch: 187/5000)[4650128 samples] loss: 0.696.\n[226 seconds](epoch: 188/5000)[4675128 samples] loss: 0.695.\n[227 seconds](epoch: 189/5000)[4700128 samples] loss: 0.694.\n[228 seconds](epoch: 190/5000)[4725128 samples] loss: 0.686.\n[229 seconds](epoch: 190/5000)[4737928 samples] loss: 0.704.\n[230 seconds](epoch: 191/5000)[4750128 samples] loss: 0.693.\n[231 seconds](epoch: 192/5000)[4775128 samples] loss: 0.688.\n[232 seconds](epoch: 192/5000)[4791128 samples] loss: 0.696.\n[232 seconds](epoch: 193/5000)[4800128 samples] loss: 0.698.\n[233 seconds](epoch: 194/5000)[4825128 samples] loss: 0.695.\n[234 seconds](epoch: 195/5000)[4850128 samples] loss: 0.696.\n[236 seconds](epoch: 196/5000)[4875128 samples] loss: 0.690.\n[237 seconds](epoch: 197/5000)[4900128 samples] loss: 0.687.\n[238 seconds](epoch: 198/5000)[4925128 samples] loss: 0.700.\n[239 seconds](epoch: 199/5000)[4950128 samples] loss: 0.693.\n[240 seconds](epoch: 200/5000)[4975128 samples] loss: 0.690.\n[241 seconds](epoch: 200/5000)[4975768 samples] loss: 0.701.\n[241 seconds](epoch: 200/5000)[4976408 samples] loss: 0.691.\n[241 seconds](epoch: 200/5000)[4977048 samples] loss: 0.697.\n[241 seconds](epoch: 200/5000)[4977688 samples] loss: 0.688.\n[241 seconds](epoch: 200/5000)[4978328 samples] loss: 0.685.\n[241 seconds](epoch: 200/5000)[4978968 samples] loss: 0.693.\n[241 seconds](epoch: 200/5000)[4979608 samples] loss: 0.694.\n[241 seconds](epoch: 200/5000)[4980248 samples] loss: 0.693.\n[241 seconds](epoch: 200/5000)[4980888 samples] loss: 0.689.\n[241 seconds](epoch: 200/5000)[4981528 samples] loss: 0.693.\n[241 seconds](epoch: 200/5000)[4982168 samples] loss: 0.694.\n[241 seconds](epoch: 200/5000)[4982808 samples] loss: 0.688.\n[241 seconds](epoch: 200/5000)[4983448 samples] loss: 0.701.\n[241 seconds](epoch: 200/5000)[4984088 samples] loss: 0.694.\n[241 seconds](epoch: 200/5000)[4984728 samples] loss: 0.684.\n[241 seconds](epoch: 200/5000)[4985368 samples] loss: 0.702.\n[241 seconds](epoch: 200/5000)[4986008 samples] loss: 0.692.\n[241 seconds](epoch: 200/5000)[4986648 samples] loss: 0.692.\n[241 seconds](epoch: 200/5000)[4987288 samples] loss: 0.689.\n[241 seconds](epoch: 200/5000)[4987928 samples] loss: 0.694.\n[241 seconds](epoch: 200/5000)[4988568 samples] loss: 0.697.\n[241 seconds](epoch: 200/5000)[4989208 samples] loss: 0.696.\n[241 seconds](epoch: 200/5000)[4989848 samples] loss: 0.703.\n[241 seconds](epoch: 200/5000)[4990488 samples] loss: 0.690.\n[241 seconds](epoch: 200/5000)[4991128 samples] loss: 0.693.\n[241 seconds](epoch: 200/5000)[4991768 samples] loss: 0.689.\n[241 seconds](epoch: 200/5000)[4992408 samples] loss: 0.706.\n[241 seconds](epoch: 200/5000)[4993048 samples] loss: 0.707.\n[241 seconds](epoch: 200/5000)[4993688 samples] loss: 0.701.\n[241 seconds](epoch: 200/5000)[4994328 samples] loss: 0.700.\n[242 seconds](epoch: 200/5000)[4994968 samples] loss: 0.694.\n[242 seconds](epoch: 200/5000)[4995608 samples] loss: 0.701.\n[242 seconds](epoch: 200/5000)[4996248 samples] loss: 0.695.\n[242 seconds](epoch: 200/5000)[4996888 samples] loss: 0.688.\n[242 seconds](epoch: 200/5000)[4997528 samples] loss: 0.689.\n[242 seconds](epoch: 200/5000)[4998168 samples] loss: 0.693.\n[242 seconds](epoch: 200/5000)[4998808 samples] loss: 0.701.\n[242 seconds](epoch: 200/5000)[4999448 samples] loss: 0.699.\n[242 seconds](epoch: 200/5000)[5000000 samples] loss: 0.692.\n[242 seconds](epoch: 201/5000)[5000128 samples] loss: 0.697.\n[243 seconds](epoch: 202/5000)[5025128 samples] loss: 0.695.\n[244 seconds](epoch: 203/5000)[5050128 samples] loss: 0.696.\n[245 seconds](epoch: 204/5000)[5075128 samples] loss: 0.697.\n[247 seconds](epoch: 205/5000)[5100128 samples] loss: 0.687.\n[248 seconds](epoch: 206/5000)[5125128 samples] loss: 0.706.\n[249 seconds](epoch: 207/5000)[5150128 samples] loss: 0.694.\n[250 seconds](epoch: 208/5000)[5175128 samples] loss: 0.697.\n[251 seconds](epoch: 208/5000)[5191128 samples] loss: 0.689.\n[251 seconds](epoch: 209/5000)[5200128 samples] loss: 0.690.\n[253 seconds](epoch: 210/5000)[5225128 samples] loss: 0.697.\n[253 seconds](epoch: 210/5000)[5237928 samples] loss: 0.700.\n[254 seconds](epoch: 211/5000)[5250128 samples] loss: 0.693.\n[255 seconds](epoch: 212/5000)[5275128 samples] loss: 0.696.\n[256 seconds](epoch: 213/5000)[5300128 samples] loss: 0.693.\n[257 seconds](epoch: 214/5000)[5325128 samples] loss: 0.689.\n[259 seconds](epoch: 215/5000)[5350128 samples] loss: 0.691.\n[260 seconds](epoch: 216/5000)[5375128 samples] loss: 0.690.\n[261 seconds](epoch: 216/5000)[5391128 samples] loss: 0.700.\n"
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-110026f25cd4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextCNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_to_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-31-42fa0406eec3>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, config, train_dataloader, device, check_interval)\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mcounts\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriteria\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# put everything together\n",
    "import time\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "config = Config()\n",
    "\n",
    "pos_data_folder = os.path.join(dataset_folder_path, \"train/pos\")\n",
    "neg_data_folder = os.path.join(dataset_folder_path, \"train/neg\")\n",
    "train_dataset = MovieReviewDataset(config, pos_data_folder, neg_data_folder, word_to_id, \n",
    "                            transform=transforms.Compose([\n",
    "                                CutOrPadTransform(config), WordsToIdsTransform(config, word_to_id)\n",
    "                            ]))\n",
    "dataloader = DataLoader(dataset=train_dataset, batch_size=config.batch_size, shuffle=True)\n",
    "\n",
    "model = TextCNN(config, len(word_to_id)).to(device)\n",
    "\n",
    "train(model, config, dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "dataset shape: (559999, 3)\n1                                                                                                                                                                                                                                                                                                           1\nE. D. Abbott Ltd                                                                                                                                                                                                                                                                               Schwan-Stabilo\n Abbott of Farnham E D Abbott Limited was a British coachbuilding business based in Farnham Surrey trading under that name from 1929. A major part of their output was under sub-contract to motor vehicle manufacturers. Their business closed in 1972.     Schwan-STABILO is a German maker of pens for ...\nName: 0, dtype: object\n"
    }
   ],
   "source": [
    "dataset = pd.read_csv(os.path.join(dataset_folder_path, \"train.csv\"))\n",
    "print(\"dataset shape:\", dataset.shape)\n",
    "print(dataset.iloc[0])\n",
    "# print(dataset.iloc[0][0])\n",
    "# print(dataset.iloc[0][1])\n",
    "# print(dataset.iloc[0][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}