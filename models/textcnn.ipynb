{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "/home/yxjiang/source/ml_playground\n"
    }
   ],
   "source": [
    "# env setup\n",
    "import os\n",
    "import sys\n",
    "\n",
    "os.chdir(\"/home/yxjiang/source/ml_playground\")\n",
    "\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Destination folder [/tmp/data] exists.\nTarget file [aclImdb_v1.tar.gz] exists, skip downloading.\nStart to extract [/tmp/data/aclImdb_v1.tar.gz] to [/tmp/data]...\nFile extracted\nProcessing vocabulary from [/tmp/data/aclImdb].\nThere size of vocabulary is : 89527\n"
    }
   ],
   "source": [
    "# data downloading\n",
    "from util import data_util\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# dataset_url=\"https://s3.amazonaws.com/fast-ai-nlp/dbpedia_csv.tgz\"\n",
    "dataset_url=\"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "\n",
    "dest_dir = \"/tmp/data\"\n",
    "dataset_folder_path = os.path.join(dest_dir, \"aclImdb\")\n",
    "data_util.download_data(url=dataset_url, dest_dir=dest_dir)\n",
    "\n",
    "# generate word to id mapping\n",
    "word_to_id, word_list = data_util.get_vocabulary(folder_path=dataset_folder_path, file_suffix=\"vocab\")\n",
    "print(\"There size of vocabulary is :\", len(word_to_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform, dataset and dataloader\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "class CutOrPadTransform:\n",
    "    \"\"\"\n",
    "    Shape all sentences to the equal length.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "    \n",
    "    def __call__(self, input):\n",
    "        if len(input[\"words\"]) >= config.sentence_max_length:\n",
    "            input[\"words\"] = input[\"words\"][:config.sentence_max_length]\n",
    "        else:\n",
    "            input[\"words\"].extend([\" \"] * (config.sentence_max_length - len(input[\"words\"])))\n",
    "        return input\n",
    "\n",
    "\n",
    "class WordsToIdsTransform:\n",
    "    \"\"\"\n",
    "    Convert the list of words to embeddings.\n",
    "    \"\"\"\n",
    "    def __init__(self, config, word_to_id):\n",
    "        self.config = config\n",
    "        self.word_to_id = word_to_id\n",
    "    \n",
    "    def __call__(self, input):\n",
    "        input[\"word_ids\"] = torch.tensor([self.word_to_id[w.lower()] for w in input[\"words\"]], dtype=torch.long)\n",
    "        # del input['words']\n",
    "        return input\n",
    "\n",
    "\n",
    "class MovieReviewDataset(Dataset):\n",
    "    def __init__(self, config, pos_data_folder, neg_data_folder, word_to_id, transform):\n",
    "        self.config = config\n",
    "        self.word_to_id = word_to_id\n",
    "        self.data = []\n",
    "        # read all data into memory\n",
    "        for filename in os.listdir(pos_data_folder):\n",
    "            if filename.endswith(\".txt\"):\n",
    "                with open(os.path.join(pos_data_folder, filename), \"r\") as f:\n",
    "                    self.data.append((f.readline(), 1))\n",
    "\n",
    "        for filename in os.listdir(neg_data_folder):\n",
    "            if filename.endswith(\".txt\"):\n",
    "                with open(os.path.join(neg_data_folder, filename), \"r\") as f:\n",
    "                    self.data.append((f.readline(), 0))\n",
    "\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        words = [w.strip() for w in self.data[idx][0].strip().split(\" \")]\n",
    "        label = self.data[idx][1]\n",
    "        input = self.transform({\"words\": words, \"label\": label})\n",
    "        # print(input[\"words\"], \"\\n\", input[\"word_ids\"], \"\\n\", input[\"label\"])\n",
    "        return input[\"words\"], input[\"word_ids\"], input[\"label\"]\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, config, vocabulary_size):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embed = nn.Embedding(vocabulary_size, config.word_embedding_length)\n",
    "        self.hidden_size = 128\n",
    "        self.num_layers = 1\n",
    "        self.directions = 1\n",
    "        self.rnn = nn.RNN(input_size=config.word_embedding_length, hidden_size=self.hidden_size, num_layers=self.num_layers)\n",
    "        self.fc1 = nn.Linear(in_features=self.hidden_size, out_features=64)\n",
    "        self.fc2 = nn.Linear(in_features=64, out_features=config.num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch = x.shape[0]\n",
    "        x = self.embed(x)  # (batch, sentence_length, embedding_dim)\n",
    "        x = x.permute(1, 0, 2).contiguous()  # (sentence_length, batch, embedding_dim)\n",
    "\n",
    "        h0 = torch.zeros((self.num_layers * self.directions, batch, self.hidden_size)).to(device)\n",
    "        output, ht = self.rnn(x, h0)\n",
    "        ht = ht.permute(1, 0, 2)  # (batch, num_layer * directions, embedding_dim)\n",
    "        ht = ht.contiguous().view(batch, self.num_layers * self.directions, self.hidden_size)\n",
    "        x = F.relu(self.fc1(ht))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = x.view(batch, -1)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, config, vocabulary_size, conv_layer_sizes):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embed = nn.Embedding(vocabulary_size, config.word_embedding_length)\n",
    "\n",
    "        self.convs = []\n",
    "        self.max_over_time_pooling = []\n",
    "        for size in conv_layer_sizes:\n",
    "            self.convs.append(nn.Conv2d(1, 1, kernel_size=(size, config.word_embedding_length)).to(device))\n",
    "            self.max_over_time_pooling.append(nn.MaxPool2d((config.sentence_max_length - size + 1, 1)).to(device))\n",
    "\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        self.fc = nn.Linear(len(self.convs), config.num_classes)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch = x.shape[0]\n",
    "        x = torch.unsqueeze(self.embed(x), 1)  # [NCHW], add channel to dimension 1\n",
    "        # convs\n",
    "        xs = []\n",
    "        for i in range(len(self.convs)):\n",
    "            xs.append(self.config.activation(self.convs[i](x)))  # conv modules\n",
    "            xs[i] = self.max_over_time_pooling[i](xs[i])  # max over time pooling modules\n",
    "\n",
    "        x = torch.cat(xs, dim=-1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        x = x.view(batch, -1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "def train(model, config, train_dataloader, test_dataloader, device, check_interval=5000):\n",
    "    criteria = config.criteria()\n",
    "    optimizer = config.optimizer(model.parameters(), config.lr)\n",
    "    start = time.time()\n",
    "    counts = 0\n",
    "    writer = SummaryWriter()\n",
    "    graph_added = False\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    for epoch in range(config.epochs):\n",
    "        for i, (words, word_ids, labels) in enumerate(train_dataloader):\n",
    "            # if not graph_added:\n",
    "            #     writer.add_graph(model, word_ids.to(device))\n",
    "            counts += labels.shape[0]\n",
    "            optimizer.zero_grad()\n",
    "            output = model(word_ids.to(device))\n",
    "            loss = criteria(output, labels.to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if ((epoch + 1) * i) % check_interval == 0:\n",
    "                print(\"[%d seconds](epoch: %d/%d)[%d samples] loss: %.3f.\" % (time.time() - start, epoch + 1, config.epochs, counts, loss.mean().item()))\n",
    "                # eval on test dataset\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    acc_eval_loss = 0.0\n",
    "                    batches = 0\n",
    "                    correct = 0\n",
    "                    total = 0\n",
    "                    for j, (words, eval_word_ids, eval_labels) in enumerate(test_dataloader):\n",
    "                        eval_output = model(eval_word_ids.to(device))\n",
    "                        eval_labels = eval_labels.to(device)\n",
    "                        eval_loss = criteria(eval_output, eval_labels)\n",
    "                        acc_eval_loss += eval_loss.item()\n",
    "                        total += eval_labels.shape[0]\n",
    "                        correct += torch.sum(torch.argmax(eval_output, dim=1) == eval_labels).cpu().numpy()\n",
    "                        batches += 1\n",
    "                    print('[%d] eval loss: %.3f, accuracy: %.3f' % ((epoch + 1) * i, acc_eval_loss / batches, correct / total))\n",
    "                    writer.add_scalar('Loss/eval', acc_eval_loss / batches, (epoch + 1))\n",
    "                model.train()\n",
    "                writer.add_scalar('Loss/train', loss.mean().item(), (epoch + 1))\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configs\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.num_classes = 2\n",
    "        self.sentence_max_length = 30\n",
    "        self.word_embedding_length = 32\n",
    "        self.activation = F.relu\n",
    "        self.criteria = nn.CrossEntropyLoss\n",
    "        self.optimizer = optim.Adam\n",
    "        self.lr = 0.005\n",
    "        self.epochs = 100\n",
    "        self.batch_size = 1024\n",
    "        self.dropout = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[0 seconds](epoch: 1/100)[1024 samples] loss: 0.780.\n[0] eval loss: 0.731, accuracy: 0.501\n[2 seconds](epoch: 2/100)[26024 samples] loss: 0.718.\n[0] eval loss: 0.693, accuracy: 0.512\n[4 seconds](epoch: 3/100)[51024 samples] loss: 0.695.\n[0] eval loss: 0.691, accuracy: 0.521\n[7 seconds](epoch: 4/100)[76024 samples] loss: 0.691.\n[0] eval loss: 0.691, accuracy: 0.537\n[9 seconds](epoch: 5/100)[101024 samples] loss: 0.684.\n[0] eval loss: 0.690, accuracy: 0.544\n[12 seconds](epoch: 6/100)[126024 samples] loss: 0.685.\n[0] eval loss: 0.689, accuracy: 0.544\n[14 seconds](epoch: 7/100)[151024 samples] loss: 0.675.\n[0] eval loss: 0.685, accuracy: 0.557\n[16 seconds](epoch: 8/100)[176024 samples] loss: 0.678.\n[0] eval loss: 0.681, accuracy: 0.568\n[19 seconds](epoch: 9/100)[201024 samples] loss: 0.656.\n[0] eval loss: 0.675, accuracy: 0.577\n[21 seconds](epoch: 10/100)[226024 samples] loss: 0.637.\n[0] eval loss: 0.666, accuracy: 0.597\n[24 seconds](epoch: 11/100)[251024 samples] loss: 0.631.\n[0] eval loss: 0.656, accuracy: 0.611\n[26 seconds](epoch: 12/100)[276024 samples] loss: 0.602.\n[0] eval loss: 0.647, accuracy: 0.621\n[29 seconds](epoch: 13/100)[301024 samples] loss: 0.569.\n[0] eval loss: 0.639, accuracy: 0.627\n[31 seconds](epoch: 14/100)[326024 samples] loss: 0.546.\n[0] eval loss: 0.632, accuracy: 0.632\n[33 seconds](epoch: 15/100)[351024 samples] loss: 0.531.\n[0] eval loss: 0.627, accuracy: 0.637\n[36 seconds](epoch: 16/100)[376024 samples] loss: 0.496.\n[0] eval loss: 0.625, accuracy: 0.639\n[38 seconds](epoch: 17/100)[401024 samples] loss: 0.481.\n[0] eval loss: 0.623, accuracy: 0.642\n[41 seconds](epoch: 18/100)[426024 samples] loss: 0.449.\n[0] eval loss: 0.623, accuracy: 0.644\n[43 seconds](epoch: 19/100)[451024 samples] loss: 0.408.\n[0] eval loss: 0.624, accuracy: 0.645\n[45 seconds](epoch: 20/100)[476024 samples] loss: 0.418.\n[0] eval loss: 0.627, accuracy: 0.646\n[48 seconds](epoch: 21/100)[501024 samples] loss: 0.390.\n[0] eval loss: 0.629, accuracy: 0.646\n[50 seconds](epoch: 22/100)[526024 samples] loss: 0.373.\n[0] eval loss: 0.633, accuracy: 0.645\n[53 seconds](epoch: 23/100)[551024 samples] loss: 0.367.\n[0] eval loss: 0.637, accuracy: 0.644\n[55 seconds](epoch: 24/100)[576024 samples] loss: 0.322.\n[0] eval loss: 0.642, accuracy: 0.645\n[58 seconds](epoch: 25/100)[601024 samples] loss: 0.331.\n[0] eval loss: 0.647, accuracy: 0.645\n[60 seconds](epoch: 26/100)[626024 samples] loss: 0.321.\n[0] eval loss: 0.652, accuracy: 0.645\n[63 seconds](epoch: 27/100)[651024 samples] loss: 0.300.\n[0] eval loss: 0.657, accuracy: 0.646\n[65 seconds](epoch: 28/100)[676024 samples] loss: 0.294.\n[0] eval loss: 0.662, accuracy: 0.644\n[67 seconds](epoch: 29/100)[701024 samples] loss: 0.298.\n[0] eval loss: 0.668, accuracy: 0.646\n[70 seconds](epoch: 30/100)[726024 samples] loss: 0.273.\n[0] eval loss: 0.674, accuracy: 0.645\n[72 seconds](epoch: 31/100)[751024 samples] loss: 0.282.\n[0] eval loss: 0.680, accuracy: 0.647\n[75 seconds](epoch: 32/100)[776024 samples] loss: 0.291.\n[0] eval loss: 0.684, accuracy: 0.648\n[77 seconds](epoch: 33/100)[801024 samples] loss: 0.263.\n[0] eval loss: 0.691, accuracy: 0.646\n[79 seconds](epoch: 34/100)[826024 samples] loss: 0.262.\n[0] eval loss: 0.697, accuracy: 0.646\n[82 seconds](epoch: 35/100)[851024 samples] loss: 0.238.\n[0] eval loss: 0.704, accuracy: 0.646\n[84 seconds](epoch: 36/100)[876024 samples] loss: 0.240.\n[0] eval loss: 0.709, accuracy: 0.645\n[87 seconds](epoch: 37/100)[901024 samples] loss: 0.235.\n[0] eval loss: 0.716, accuracy: 0.645\n[89 seconds](epoch: 38/100)[926024 samples] loss: 0.245.\n[0] eval loss: 0.722, accuracy: 0.645\n[92 seconds](epoch: 39/100)[951024 samples] loss: 0.244.\n[0] eval loss: 0.727, accuracy: 0.645\n[94 seconds](epoch: 40/100)[976024 samples] loss: 0.208.\n[0] eval loss: 0.735, accuracy: 0.645\n[96 seconds](epoch: 41/100)[1001024 samples] loss: 0.225.\n[0] eval loss: 0.740, accuracy: 0.644\n[99 seconds](epoch: 42/100)[1026024 samples] loss: 0.215.\n[0] eval loss: 0.746, accuracy: 0.644\n[101 seconds](epoch: 43/100)[1051024 samples] loss: 0.218.\n[0] eval loss: 0.753, accuracy: 0.644\n[104 seconds](epoch: 44/100)[1076024 samples] loss: 0.193.\n[0] eval loss: 0.759, accuracy: 0.645\n[106 seconds](epoch: 45/100)[1101024 samples] loss: 0.202.\n[0] eval loss: 0.766, accuracy: 0.643\n[108 seconds](epoch: 46/100)[1126024 samples] loss: 0.202.\n[0] eval loss: 0.772, accuracy: 0.643\n[111 seconds](epoch: 47/100)[1151024 samples] loss: 0.192.\n[0] eval loss: 0.777, accuracy: 0.643\n[113 seconds](epoch: 48/100)[1176024 samples] loss: 0.188.\n[0] eval loss: 0.785, accuracy: 0.642\n[116 seconds](epoch: 49/100)[1201024 samples] loss: 0.204.\n[0] eval loss: 0.790, accuracy: 0.643\n[118 seconds](epoch: 50/100)[1226024 samples] loss: 0.222.\n[0] eval loss: 0.799, accuracy: 0.643\n[120 seconds](epoch: 51/100)[1251024 samples] loss: 0.174.\n[0] eval loss: 0.805, accuracy: 0.642\n[123 seconds](epoch: 52/100)[1276024 samples] loss: 0.214.\n[0] eval loss: 0.812, accuracy: 0.643\n"
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-e25e42462168>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# model = RNN(config, len(word_list)).to(device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-65-e0003472b3dd>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, config, train_dataloader, test_dataloader, device, check_interval)\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mcounts\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriteria\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-34-f1dd49e15c57>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0mxs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# conv modules\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m             \u001b[0mxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_over_time_pooling\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# max over time pooling modules\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   1061\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1063\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1064\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# put everything together\n",
    "import time\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "config = Config()\n",
    "\n",
    "pos_train_data_folder = os.path.join(dataset_folder_path, \"train/pos\")\n",
    "neg_train_data_folder = os.path.join(dataset_folder_path, \"train/neg\")\n",
    "train_dataset = MovieReviewDataset(config, pos_train_data_folder, neg_train_data_folder, word_to_id, \n",
    "                            transform=transforms.Compose([\n",
    "                                CutOrPadTransform(config), WordsToIdsTransform(config, word_to_id)\n",
    "                            ]))\n",
    "\n",
    "pos_test_data_folder = os.path.join(dataset_folder_path, \"test/pos\")\n",
    "neg_test_data_folder = os.path.join(dataset_folder_path, \"test/neg\")\n",
    "test_dataset = MovieReviewDataset(config, pos_test_data_folder, neg_test_data_folder, word_to_id, \n",
    "                            transform=transforms.Compose([\n",
    "                                CutOrPadTransform(config), WordsToIdsTransform(config, word_to_id)\n",
    "                            ]))\n",
    "\n",
    "train_dataloader = DataLoader(dataset=train_dataset, batch_size=config.batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(dataset=test_dataset, batch_size=config.batch_size)\n",
    "\n",
    "model = TextCNN(config, len(word_to_id), conv_layer_sizes=(3, 4, 5)).to(device)\n",
    "# model = RNN(config, len(word_list)).to(device)\n",
    "\n",
    "train(model, config, train_dataloader, test_dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "sentence_length = 5\n",
    "batch_size = 3\n",
    "input_dim = 6\n",
    "num_direction = 1\n",
    "layer = 1\n",
    "hidden_size = 7\n",
    "\n",
    "rnn = nn.RNN(input_dim, hidden_size, layer)\n",
    "x = torch.randn(sentence_length, batch_size, input_dim)\n",
    "h0 = torch.randn(num_direction * layer, batch_size, hidden_size)\n",
    "\n",
    "output, hn = rnn(x, h0)\n",
    "print(output.shape, hn.shape)\n",
    "\n",
    "print(hn)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}