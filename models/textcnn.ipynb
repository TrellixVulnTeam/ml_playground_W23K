{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "/home/yxjiang/source/ml_playground\n"
    }
   ],
   "source": [
    "# env setup\n",
    "import os\n",
    "import sys\n",
    "\n",
    "os.chdir(\"/home/yxjiang/source/ml_playground\")\n",
    "\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Destination folder [/tmp/data] exists.\nTarget file [aclImdb_v1.tar.gz] exists, skip downloading.\nStart to extract [/tmp/data/aclImdb_v1.tar.gz] to [/tmp/data]...\nFile extracted\nProcessing vocabulary from [/tmp/data/aclImdb].\nThere size of vocabulary is : 89527\n"
    }
   ],
   "source": [
    "# data downloading\n",
    "from util import data_util\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# dataset_url=\"https://s3.amazonaws.com/fast-ai-nlp/dbpedia_csv.tgz\"\n",
    "dataset_url=\"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "\n",
    "dest_dir = \"/tmp/data\"\n",
    "dataset_folder_path = os.path.join(dest_dir, \"aclImdb\")\n",
    "data_util.download_data(url=dataset_url, dest_dir=dest_dir)\n",
    "\n",
    "# generate word to id mapping\n",
    "word_to_id = data_util.get_vocabulary(folder_path=dataset_folder_path, file_suffix=\"vocab\")\n",
    "print(\"There size of vocabulary is :\", len(word_to_id))\n",
    "\n",
    "# generate class id to name mapping\n",
    "# class_to_name = defaultdict(str)\n",
    "# with open(os.path.join(dataset_folder_path, \"classes.txt\"), \"r\") as f:\n",
    "#     for i, class_name in enumerate(f):\n",
    "#         class_to_name[i] = class_name.strip()\n",
    "# print(\"There class mapping:\", class_to_name.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configs\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.num_classes = 2\n",
    "        self.sentence_max_length = 20\n",
    "        self.word_embedding_length = 32\n",
    "        self.activation = F.relu\n",
    "        self.criteria = nn.CrossEntropyLoss\n",
    "        self.optimizer = optim.SGD\n",
    "        self.lr = 0.001\n",
    "        self.epochs = 5000\n",
    "        self.batch_size = 128\n",
    "        self.dropout = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform, dataset and dataloader\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "class CutOrPadTransform:\n",
    "    \"\"\"\n",
    "    Shape all sentences to the equal length.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "    \n",
    "    def __call__(self, input):\n",
    "        if len(input['words']) >= config.sentence_max_length:\n",
    "            input['words'] = input['words'][:config.sentence_max_length]\n",
    "        else:\n",
    "            input['words'].extend([' '] * (config.sentence_max_length - len(input['words'])))\n",
    "        return input\n",
    "\n",
    "\n",
    "class WordsToIdsTransform:\n",
    "    \"\"\"\n",
    "    Convert the list of words to embeddings.\n",
    "    \"\"\"\n",
    "    def __init__(self, config, word_to_id):\n",
    "        self.config = config\n",
    "        self.word_to_id = word_to_id\n",
    "    \n",
    "    def __call__(self, input):\n",
    "        input['word_ids'] = torch.tensor([self.word_to_id[w.lower()] for w in input['words']], dtype=torch.long)\n",
    "        # del input['words']\n",
    "        return input\n",
    "\n",
    "\n",
    "class MovieReviewDataset(Dataset):\n",
    "    def __init__(self, config, pos_data_folder, neg_data_folder, word_to_id, transform):\n",
    "        self.config = config\n",
    "        self.word_to_id = word_to_id\n",
    "        self.data = []\n",
    "        # read all data into memory\n",
    "        for filename in os.listdir(pos_data_folder):\n",
    "            if filename.endswith(\".txt\"):\n",
    "                with open(os.path.join(pos_data_folder, filename), \"r\") as f:\n",
    "                    self.data.append((f.readline(), 1))\n",
    "\n",
    "        for filename in os.listdir(pos_data_folder):\n",
    "            if filename.endswith(\".txt\"):\n",
    "                with open(os.path.join(pos_data_folder, filename), \"r\") as f:\n",
    "                    self.data.append((f.readline(), 0))\n",
    "\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        words = [w.strip() for w in self.data[idx][0].strip().split(\" \")]\n",
    "        label = self.data[idx][1]\n",
    "        input = self.transform({'words': words, 'label': label})\n",
    "        # print(input['words'], input['word_ids'], input['label'])\n",
    "        return input['words'], input['word_ids'], input['label']\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, config, vocabulary_size):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embed = nn.Embedding(vocabulary_size, config.word_embedding_length)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(1, 1, kernel_size=(3, config.word_embedding_length))\n",
    "        self.conv4 = nn.Conv2d(1, 1, kernel_size=(4, config.word_embedding_length))\n",
    "        self.conv5 = nn.Conv2d(1, 1, kernel_size=(5, config.word_embedding_length))\n",
    "        self.conv6 = nn.Conv2d(1, 1, kernel_size=(6, config.word_embedding_length))\n",
    "        self.conv7 = nn.Conv2d(1, 1, kernel_size=(7, config.word_embedding_length))\n",
    "\n",
    "        self.max_over_time_pool3 = nn.MaxPool2d((config.sentence_max_length - 2, 1))\n",
    "        self.max_over_time_pool4 = nn.MaxPool2d((config.sentence_max_length - 3, 1))\n",
    "        self.max_over_time_pool5 = nn.MaxPool2d((config.sentence_max_length - 4, 1))\n",
    "        self.max_over_time_pool6 = nn.MaxPool2d((config.sentence_max_length - 5, 1))\n",
    "        self.max_over_time_pool7 = nn.MaxPool2d((config.sentence_max_length - 6, 1))\n",
    "\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        self.fc = nn.Linear(5, config.num_classes)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch = x.shape[0]\n",
    "        x = torch.unsqueeze(self.embed(x), 1)  # [NCHW]\n",
    "        c = self.conv3(x)\n",
    "        # convs\n",
    "        x1 = self.config.activation(self.conv3(x))\n",
    "        x2 = self.config.activation(self.conv4(x))\n",
    "        x3 = self.config.activation(self.conv5(x))\n",
    "        x4 = self.config.activation(self.conv6(x))\n",
    "        x5 = self.config.activation(self.conv7(x))\n",
    "\n",
    "        # max over time pooling\n",
    "        x1 = self.max_over_time_pool3(x1)\n",
    "        x2 = self.max_over_time_pool4(x2)\n",
    "        x3 = self.max_over_time_pool5(x3)\n",
    "        x4 = self.max_over_time_pool6(x4)\n",
    "        x5 = self.max_over_time_pool7(x5)\n",
    "\n",
    "        x = torch.cat((x1, x2, x3, x4, x5), dim=-1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        x = x.view(batch, -1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer\n",
    "def train(model, config, train_dataloader, device, check_interval=1000):\n",
    "    criteria = config.criteria()\n",
    "    optimizer = config.optimizer(model.parameters(), config.lr)\n",
    "    start = time.time()\n",
    "    counts = 0\n",
    "    for epoch in range(config.epochs):\n",
    "        for i, (words, word_ids, labels) in enumerate(train_dataloader):\n",
    "            counts += labels.shape[0]\n",
    "            optimizer.zero_grad()\n",
    "            output = model(word_ids.to(device))\n",
    "            loss = criteria(output, labels.to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if ((epoch + 1) * i) % check_interval == 0:\n",
    "                print(\"[%d seconds](epoch: %d/%d)[%d samples] loss: %.3f.\" % (time.time() - start, epoch + 1, config.epochs, counts, loss.mean().item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[0 seconds](epoch: 1/5000)[128 samples] loss: 0.749.\n[1 seconds](epoch: 2/5000)[25128 samples] loss: 0.691.\n[2 seconds](epoch: 3/5000)[50128 samples] loss: 0.700.\n[4 seconds](epoch: 4/5000)[75128 samples] loss: 0.690.\n[5 seconds](epoch: 5/5000)[100128 samples] loss: 0.699.\n[7 seconds](epoch: 6/5000)[125128 samples] loss: 0.691.\n[8 seconds](epoch: 7/5000)[150128 samples] loss: 0.692.\n[10 seconds](epoch: 8/5000)[175128 samples] loss: 0.688.\n[11 seconds](epoch: 8/5000)[191128 samples] loss: 0.694.\n[11 seconds](epoch: 9/5000)[200128 samples] loss: 0.695.\n[13 seconds](epoch: 10/5000)[225128 samples] loss: 0.692.\n[14 seconds](epoch: 10/5000)[237928 samples] loss: 0.693.\n[14 seconds](epoch: 11/5000)[250128 samples] loss: 0.691.\n[16 seconds](epoch: 12/5000)[275128 samples] loss: 0.691.\n[17 seconds](epoch: 13/5000)[300128 samples] loss: 0.694.\n[19 seconds](epoch: 14/5000)[325128 samples] loss: 0.691.\n[20 seconds](epoch: 15/5000)[350128 samples] loss: 0.694.\n[22 seconds](epoch: 16/5000)[375128 samples] loss: 0.695.\n[23 seconds](epoch: 16/5000)[391128 samples] loss: 0.694.\n[23 seconds](epoch: 17/5000)[400128 samples] loss: 0.694.\n[25 seconds](epoch: 18/5000)[425128 samples] loss: 0.695.\n[26 seconds](epoch: 19/5000)[450128 samples] loss: 0.691.\n[28 seconds](epoch: 20/5000)[475128 samples] loss: 0.694.\n[28 seconds](epoch: 20/5000)[481528 samples] loss: 0.695.\n[29 seconds](epoch: 20/5000)[487928 samples] loss: 0.694.\n[29 seconds](epoch: 20/5000)[494328 samples] loss: 0.693.\n[29 seconds](epoch: 21/5000)[500128 samples] loss: 0.694.\n[31 seconds](epoch: 22/5000)[525128 samples] loss: 0.692.\n[32 seconds](epoch: 23/5000)[550128 samples] loss: 0.694.\n[34 seconds](epoch: 24/5000)[575128 samples] loss: 0.694.\n[35 seconds](epoch: 24/5000)[591128 samples] loss: 0.692.\n[35 seconds](epoch: 25/5000)[600128 samples] loss: 0.694.\n[36 seconds](epoch: 25/5000)[605248 samples] loss: 0.693.\n[36 seconds](epoch: 25/5000)[610368 samples] loss: 0.692.\n[36 seconds](epoch: 25/5000)[615488 samples] loss: 0.694.\n[37 seconds](epoch: 25/5000)[620608 samples] loss: 0.694.\n[37 seconds](epoch: 26/5000)[625128 samples] loss: 0.694.\n[38 seconds](epoch: 27/5000)[650128 samples] loss: 0.694.\n[40 seconds](epoch: 28/5000)[675128 samples] loss: 0.696.\n[41 seconds](epoch: 29/5000)[700128 samples] loss: 0.693.\n[43 seconds](epoch: 30/5000)[725128 samples] loss: 0.693.\n[44 seconds](epoch: 30/5000)[737928 samples] loss: 0.694.\n[44 seconds](epoch: 31/5000)[750128 samples] loss: 0.693.\n[46 seconds](epoch: 32/5000)[775128 samples] loss: 0.693.\n"
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-110026f25cd4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextCNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_to_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-26-ba2f1132ec9f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, config, train_dataloader, device, check_interval)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mcounts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m             \u001b[0mcounts\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-196dd54b57c1>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'words'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'label'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# put everything together\n",
    "import time\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "config = Config()\n",
    "\n",
    "pos_data_folder = os.path.join(dataset_folder_path, \"train/pos\")\n",
    "neg_data_folder = os.path.join(dataset_folder_path, \"train/neg\")\n",
    "train_dataset = MovieReviewDataset(config, pos_data_folder, neg_data_folder, word_to_id, \n",
    "                            transform=transforms.Compose([\n",
    "                                CutOrPadTransform(config), WordsToIdsTransform(config, word_to_id)\n",
    "                            ]))\n",
    "dataloader = DataLoader(dataset=train_dataset, batch_size=config.batch_size, shuffle=True)\n",
    "\n",
    "model = TextCNN(config, len(word_to_id)).to(device)\n",
    "\n",
    "train(model, config, dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(word_to_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}