{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper-parameters\n",
    "input_features = 784\n",
    "num_classes = 10\n",
    "num_epoch = 5\n",
    "batch_size = 100\n",
    "learning_rate = 0.002\n",
    "l1_weight = 0.001\n",
    "l2_weight = 0.001\n",
    "\n",
    "# load dataset\n",
    "train_data = torchvision.datasets.MNIST(root='/data', train=True, download=True, transform=transforms.ToTensor())\n",
    "test_data = torchvision.datasets.MNIST(root='/data', train=False, transform=transforms.ToTensor())\n",
    "\n",
    "# initiate data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, input_features, num_classes, num_epoch, learning_rate, l1_weight=0, l2_weight=0):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.model = nn.Linear(input_features, num_classes)\n",
    "        self.input_features = input_features\n",
    "        self.num_epoch = num_epoch\n",
    "        self.learning_rate = learning_rate\n",
    "        self.l1_weight = l1_weight\n",
    "        self.l2_weight = l2_weight\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = torch.optim.SGD(self.model.parameters(), lr=self.learning_rate)\n",
    "    \n",
    "    def train(self, train_loader, model_name='logistic_regression.model', output_log_freq=0):\n",
    "        \"\"\"\n",
    "        Train the model with given train_loader. Save the model if model name specified.\n",
    "        \"\"\"\n",
    "        total = len(train_loader)\n",
    "        for e in range(self.num_epoch):\n",
    "            for i, (instances, labels) in enumerate(train_loader):\n",
    "                instances = instances.reshape(-1, self.input_features)\n",
    "                # Forward\n",
    "                output = self.model(instances)\n",
    "                # Calculate loss\n",
    "                params = torch.cat([x.view(-1) for x in self.model.parameters()])\n",
    "                l1_loss = 0 if self.l1_weight == 0 else torch.norm(params, 1)\n",
    "                l2_loss = 0 if self.l2_weight == 0 else torch.norm(params, 2)\n",
    "                loss = self.criterion(output, labels) + self.l1_weight * l1_loss + self.l2_weight * l2_loss\n",
    "                # Update weights\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                if output_log_freq and (i + 1) % output_log_freq == 0:\n",
    "                    print('Epoch %d/%d, trained %d/%d instances, Logloss: %.5f' % \n",
    "                          (e, self.num_epoch, i + 1, total, loss.item()))\n",
    "        if model_name:\n",
    "            torch.save(self.model.state_dict(), model_name)\n",
    "            \n",
    "    def predict(self, instances):\n",
    "        \"\"\"\n",
    "        Predict the label with given training instance batch.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            instances = instances.reshape(-1, self.input_features)\n",
    "            output = self.model(instances)  # tensor with dim [batch_size, 10] \n",
    "            return torch.max(output.data, 1)[1]  # idx of the max element for each instance indicates the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/5, trained 100/600 instances, Logloss: 2.27502\n",
      "Epoch 0/5, trained 200/600 instances, Logloss: 2.09551\n",
      "Epoch 0/5, trained 300/600 instances, Logloss: 1.94368\n",
      "Epoch 0/5, trained 400/600 instances, Logloss: 1.82699\n",
      "Epoch 0/5, trained 500/600 instances, Logloss: 1.72551\n",
      "Epoch 0/5, trained 600/600 instances, Logloss: 1.59623\n",
      "Epoch 1/5, trained 100/600 instances, Logloss: 1.50132\n",
      "Epoch 1/5, trained 200/600 instances, Logloss: 1.50830\n",
      "Epoch 1/5, trained 300/600 instances, Logloss: 1.40051\n",
      "Epoch 1/5, trained 400/600 instances, Logloss: 1.28609\n",
      "Epoch 1/5, trained 500/600 instances, Logloss: 1.29996\n",
      "Epoch 1/5, trained 600/600 instances, Logloss: 1.23473\n",
      "Epoch 2/5, trained 100/600 instances, Logloss: 1.32620\n",
      "Epoch 2/5, trained 200/600 instances, Logloss: 1.19099\n",
      "Epoch 2/5, trained 300/600 instances, Logloss: 1.22588\n",
      "Epoch 2/5, trained 400/600 instances, Logloss: 1.10004\n",
      "Epoch 2/5, trained 500/600 instances, Logloss: 1.22927\n",
      "Epoch 2/5, trained 600/600 instances, Logloss: 1.14389\n",
      "Epoch 3/5, trained 100/600 instances, Logloss: 1.07697\n",
      "Epoch 3/5, trained 200/600 instances, Logloss: 1.24223\n",
      "Epoch 3/5, trained 300/600 instances, Logloss: 1.11019\n",
      "Epoch 3/5, trained 400/600 instances, Logloss: 1.04059\n",
      "Epoch 3/5, trained 500/600 instances, Logloss: 1.05674\n",
      "Epoch 3/5, trained 600/600 instances, Logloss: 1.14406\n",
      "Epoch 4/5, trained 100/600 instances, Logloss: 0.91945\n",
      "Epoch 4/5, trained 200/600 instances, Logloss: 1.04701\n",
      "Epoch 4/5, trained 300/600 instances, Logloss: 0.91218\n",
      "Epoch 4/5, trained 400/600 instances, Logloss: 1.10652\n",
      "Epoch 4/5, trained 500/600 instances, Logloss: 1.04174\n",
      "Epoch 4/5, trained 600/600 instances, Logloss: 0.98303\n",
      "Accuracy: 8536/10000\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "lr = LogisticRegression(input_features, num_classes, num_epoch, learning_rate, l1_weight, l2_weight)\n",
    "lr.train(train_loader, output_log_freq=100)\n",
    "# Evaluate\n",
    "correct, total = 0, 0\n",
    "for images, labels in test_loader:\n",
    "    total += labels.size(0)\n",
    "    predicted = lr.predict(images)\n",
    "    correct += (predicted == labels).sum()\n",
    "\n",
    "print('Accuracy: %d/%d' % (correct, total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
