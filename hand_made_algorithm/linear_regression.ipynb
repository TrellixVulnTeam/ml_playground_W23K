{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression is a predictive model that assumes the outcome has the linear relationship with the observed signals, e.g. $\\hat{y} = w^Tx$, where $\\hat{y}$ is the outcome predicted with respect to the given observation vector $x = (x_1, x_2, ..., x_k)$ and the learned weights of the model $w = (w_1, w_2, ..., w_k)$.\n",
    "\n",
    "![linear regression](https://github.com/yxjiang/ml_playground/blob/master/hand_made_algorithm/imgs/linear_regression.png?raw=true)\n",
    "\n",
    "To quantify the predictability of the model, min-squared error (MSE) is used as the cost function is defined as $C = -\\frac{1}{2n}\\sum_m (y^{(m)} - \\hat{y^{(m)}})^2$. Usually, a regularization term $b = \\lambda ||w||_m$ is added to make less likely to overfit to the training data, making the cost function. Therefore the cost function is $C = \\frac{1}{2n} \\sum_m (y^{(m)} - \\hat{y^{(m)}}) + \\frac{\\lambda}{2}||w||_2^2$, and the partial derivative for each weight $w_i$ is $-(y - \\hat{y})x_i + \\lambda w_i$. If the weight update is conducted in mini-batch, the weight update for $w_i$ would be $-\\sum_m (y^{(m)} - \\hat{y^{(m)}})x_i + \\lambda\\sum_m w_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LinearRegression:\n",
    "    \n",
    "    def __init__(self, ndim, l2_weight):\n",
    "        self.W = np.random.randn(1, ndim + 1)  # to include the weight for the bias term\n",
    "        self.l2_weight = l2_weight\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\" Predict given a batch of inputs.\"\"\"\n",
    "        bias = np.ones((X.shape[0], 1))  # pretend 1 to X as the bias term\n",
    "        X = np.concatenate((bias, X), axis=1)\n",
    "        return X.dot(self.W.T)  # y = w^T * x\n",
    "    \n",
    "    def train(self, X, y, lr):\n",
    "        \"\"\" Update the model weights given a batch of training instances.\"\"\"\n",
    "        outputs = self.predict(X)  # dim (10, 1)\n",
    "        diff = -(np.expand_dims(y, axis=1) - outputs)  # dim (10, 1)\n",
    "        bias = np.ones((X.shape[0], 1))  # pretend 1 to X as the bias term\n",
    "        X_with_bias = np.concatenate((bias, X), axis=1)\n",
    "        dW = np.sum(diff * X_with_bias + self.l2_weight * self.W, axis=0)\n",
    "        self.W -= lr * dW \n",
    "        return abs(np.sum(diff) / len(diff))  # return the loss\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.predict: [[-0.93978365]\n",
      " [ 4.10153789]]\n",
      "iteration 1, loss: 0.80019\n",
      "real: -2.438, predicted: 7.998\n",
      "iteration 2, loss: 0.10137\n",
      "iteration 3, loss: 0.68592\n",
      "iteration 4, loss: 0.53506\n",
      "iteration 5, loss: 0.15001\n",
      "iteration 6, loss: 0.27850\n",
      "iteration 7, loss: 0.02209\n",
      "iteration 8, loss: 0.89841\n",
      "iteration 9, loss: 0.01167\n",
      "iteration 10, loss: 0.15910\n",
      "iteration 11, loss: 0.41305\n",
      "real: 2.878, predicted: 1.241\n",
      "iteration 12, loss: 0.46684\n",
      "iteration 13, loss: 1.09523\n",
      "iteration 14, loss: 0.63872\n",
      "iteration 15, loss: 0.53174\n",
      "iteration 16, loss: 0.26756\n",
      "iteration 17, loss: 0.14297\n",
      "iteration 18, loss: 0.27656\n",
      "iteration 19, loss: 0.10277\n",
      "iteration 20, loss: 0.24678\n",
      "iteration 21, loss: 0.38130\n",
      "real: -7.549, predicted: -7.791\n",
      "iteration 22, loss: 0.44407\n",
      "iteration 23, loss: 0.11493\n",
      "iteration 24, loss: 0.23943\n",
      "iteration 25, loss: 0.15321\n",
      "iteration 26, loss: 0.11421\n",
      "iteration 27, loss: 0.12623\n",
      "iteration 28, loss: 0.05955\n",
      "iteration 29, loss: 0.06745\n",
      "iteration 30, loss: 0.22923\n",
      "iteration 31, loss: 0.13072\n",
      "real: 2.046, predicted: 1.934\n",
      "iteration 32, loss: 0.06595\n",
      "iteration 33, loss: 0.02132\n",
      "iteration 34, loss: 0.02258\n",
      "iteration 35, loss: 0.01848\n",
      "iteration 36, loss: 0.08877\n",
      "iteration 37, loss: 0.07514\n",
      "iteration 38, loss: 0.05686\n",
      "iteration 39, loss: 0.11486\n",
      "iteration 40, loss: 0.12586\n",
      "iteration 41, loss: 0.03228\n",
      "real: -10.976, predicted: -10.801\n",
      "iteration 42, loss: 0.00147\n",
      "iteration 43, loss: 0.04544\n",
      "iteration 44, loss: 0.00855\n",
      "iteration 45, loss: 0.00755\n",
      "iteration 46, loss: 0.02235\n",
      "iteration 47, loss: 0.02786\n",
      "iteration 48, loss: 0.01409\n",
      "iteration 49, loss: 0.00945\n",
      "iteration 50, loss: 0.05002\n",
      "iteration 51, loss: 0.00912\n",
      "real: 3.046, predicted: 2.983\n",
      "iteration 52, loss: 0.02475\n",
      "iteration 53, loss: 0.00754\n",
      "iteration 54, loss: 0.01918\n",
      "iteration 55, loss: 0.01537\n",
      "iteration 56, loss: 0.01930\n",
      "iteration 57, loss: 0.02212\n",
      "iteration 58, loss: 0.00929\n",
      "iteration 59, loss: 0.01187\n",
      "iteration 60, loss: 0.02500\n",
      "iteration 61, loss: 0.02637\n",
      "real: -1.087, predicted: -1.073\n",
      "iteration 62, loss: 0.00860\n",
      "iteration 63, loss: 0.01883\n",
      "iteration 64, loss: 0.02654\n",
      "iteration 65, loss: 0.00459\n",
      "iteration 66, loss: 0.01665\n",
      "iteration 67, loss: 0.02461\n",
      "iteration 68, loss: 0.00315\n",
      "iteration 69, loss: 0.00069\n",
      "iteration 70, loss: 0.00342\n",
      "iteration 71, loss: 0.02626\n",
      "real: 10.696, predicted: 10.621\n",
      "iteration 72, loss: 0.01022\n",
      "iteration 73, loss: 0.01414\n",
      "iteration 74, loss: 0.03255\n",
      "iteration 75, loss: 0.02342\n",
      "iteration 76, loss: 0.01015\n",
      "iteration 77, loss: 0.00730\n",
      "iteration 78, loss: 0.01110\n",
      "iteration 79, loss: 0.02825\n",
      "iteration 80, loss: 0.01044\n",
      "iteration 81, loss: 0.00574\n",
      "real: 5.335, predicted: 5.274\n",
      "iteration 82, loss: 0.01530\n",
      "iteration 83, loss: 0.00762\n",
      "iteration 84, loss: 0.01621\n",
      "iteration 85, loss: 0.01798\n",
      "iteration 86, loss: 0.00925\n",
      "iteration 87, loss: 0.01538\n",
      "iteration 88, loss: 0.01761\n",
      "iteration 89, loss: 0.00140\n",
      "iteration 90, loss: 0.01023\n",
      "iteration 91, loss: 0.00436\n",
      "real: 5.555, predicted: 5.513\n",
      "iteration 92, loss: 0.02215\n",
      "iteration 93, loss: 0.00636\n",
      "iteration 94, loss: 0.00721\n",
      "iteration 95, loss: 0.00834\n",
      "iteration 96, loss: 0.01400\n",
      "iteration 97, loss: 0.00790\n",
      "iteration 98, loss: 0.02644\n",
      "iteration 99, loss: 0.00058\n",
      "iteration 100, loss: 0.00435\n"
     ]
    }
   ],
   "source": [
    "model = LinearRegression(ndim=20, l2_weight=0.01)\n",
    "X = np.random.randn(2, 20)\n",
    "print(\"model.predict:\", model.predict(X))\n",
    "\n",
    "for it in range(100):\n",
    "    X = np.random.randn(10, 20)\n",
    "    y = np.sum(X, axis=1)\n",
    "    loss = model.train(X, y, 0.01)\n",
    "    print(\"iteration %d, loss: %.5f\" % (it + 1, loss))\n",
    "    if it % 10 == 0:\n",
    "        test_X = np.random.randn(1, 20)\n",
    "        test_y = np.sum(test_X, axis=1)\n",
    "        pred_y = model.predict(test_X)\n",
    "        print(\"real: %.3f, predicted: %.3f\" % (test_y, pred_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
