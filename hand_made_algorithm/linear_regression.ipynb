{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression is a predictive model that assumes the outcome has the linear relationship with the observed signals, e.g. $\\hat{y} = w^Tx$, where $\\hat{y}$ is the outcome predicted with respect to the given observation vector $x = (x_1, x_2, ..., x_k)$ and the learned weights of the model $w = (w_1, w_2, ..., w_k)$.\n",
    "\n",
    "![linear regression](https://github.com/yxjiang/ml_playground/blob/master/hand_made_algorithm/imgs/linear_regression.png?raw=true)\n",
    "\n",
    "To quantify the predictability of the model, min-squared error (MSE) is used as the cost function is defined as $C = -\\frac{1}{2n}\\sum_m (y^{(m)} - \\hat{y^{(m)}})^2$. Usually, a regularization term $b = \\lambda ||w||_m$ is added to make less likely to overfit to the training data, making the cost function. Therefore the cost function is $C = \\frac{1}{2n} \\sum_m (y^{(m)} - \\hat{y^{(m)}}) + \\frac{\\lambda}{2}||w||_2^2$, and the partial derivative for each weight $w_i$ is $-(y - \\hat{y})x_i + \\lambda w_i$. If the weight update is conducted in mini-batch, the weight update for $w_i$ would be $-\\sum_m (y^{(m)} - \\hat{y^{(m)}})x_i + \\lambda\\sum_m w_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LinearRegression:\n",
    "    \n",
    "    def __init__(self, ndim, l2_weight):\n",
    "        self.W = np.random.randn(1, ndim + 1)  # to include the weight for the bias term\n",
    "        self.l2_weight = l2_weight\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\" Predict given a batch of inputs.\"\"\"\n",
    "        bias = np.ones((X.shape[0], 1))  # pretend 1 to X as the bias term\n",
    "        X = np.concatenate((bias, X), axis=1)\n",
    "        return X.dot(self.W.T)  # y = w^T * x\n",
    "    \n",
    "    def train(self, X, y, lr):\n",
    "        \"\"\" Update the model weights given a batch of training instances.\"\"\"\n",
    "        outputs = self.predict(X)  # dim (10, 1)\n",
    "        diff = -(np.expand_dims(y, axis=1) - outputs)  # dim (10, 1)\n",
    "        bias = np.ones((X.shape[0], 1))  # pretend 1 to X as the bias term\n",
    "        X_with_bias = np.concatenate((bias, X), axis=1)\n",
    "        dW = np.sum(diff * X_with_bias + self.l2_weight * self.W, axis=0)\n",
    "        self.W -= lr * dW \n",
    "        return abs(np.sum(diff) / len(diff))  # return the loss\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.predict: [[6.52106568]\n",
      " [3.71523878]]\n",
      "iteration 1, loss: 0.09654\n",
      "real: -1.066, predicted: 0.468\n",
      "iteration 2, loss: 2.55716\n",
      "iteration 3, loss: 1.03130\n",
      "iteration 4, loss: 1.37585\n",
      "iteration 5, loss: 0.95707\n",
      "iteration 6, loss: 1.29227\n",
      "iteration 7, loss: 2.26796\n",
      "iteration 8, loss: 2.89924\n",
      "iteration 9, loss: 0.10581\n",
      "iteration 10, loss: 1.01190\n",
      "iteration 11, loss: 1.58040\n",
      "real: 2.373, predicted: 0.391\n",
      "iteration 12, loss: 1.03029\n",
      "iteration 13, loss: 0.98611\n",
      "iteration 14, loss: 0.22386\n",
      "iteration 15, loss: 1.10428\n",
      "iteration 16, loss: 0.56167\n",
      "iteration 17, loss: 0.63822\n",
      "iteration 18, loss: 0.00166\n",
      "iteration 19, loss: 0.26458\n",
      "iteration 20, loss: 0.53435\n",
      "iteration 21, loss: 0.55318\n",
      "real: -5.440, predicted: -6.147\n",
      "iteration 22, loss: 0.04965\n",
      "iteration 23, loss: 0.40074\n",
      "iteration 24, loss: 0.44546\n",
      "iteration 25, loss: 0.12086\n",
      "iteration 26, loss: 0.11701\n",
      "iteration 27, loss: 0.38325\n",
      "iteration 28, loss: 0.37724\n",
      "iteration 29, loss: 0.02003\n",
      "iteration 30, loss: 0.04332\n",
      "iteration 31, loss: 0.21487\n",
      "real: 5.384, predicted: 5.495\n",
      "iteration 32, loss: 0.14291\n",
      "iteration 33, loss: 0.08909\n",
      "iteration 34, loss: 0.04709\n",
      "iteration 35, loss: 0.04596\n",
      "iteration 36, loss: 0.08834\n",
      "iteration 37, loss: 0.15402\n",
      "iteration 38, loss: 0.03337\n",
      "iteration 39, loss: 0.04617\n",
      "iteration 40, loss: 0.06418\n",
      "iteration 41, loss: 0.01636\n",
      "real: -0.613, predicted: -0.582\n",
      "iteration 42, loss: 0.00528\n",
      "iteration 43, loss: 0.00598\n",
      "iteration 44, loss: 0.07601\n",
      "iteration 45, loss: 0.04673\n",
      "iteration 46, loss: 0.01052\n",
      "iteration 47, loss: 0.00368\n",
      "iteration 48, loss: 0.00346\n",
      "iteration 49, loss: 0.00730\n",
      "iteration 50, loss: 0.01450\n",
      "iteration 51, loss: 0.06697\n",
      "real: 5.009, predicted: 4.883\n",
      "iteration 52, loss: 0.02500\n",
      "iteration 53, loss: 0.04033\n",
      "iteration 54, loss: 0.03196\n",
      "iteration 55, loss: 0.00891\n",
      "iteration 56, loss: 0.01916\n",
      "iteration 57, loss: 0.00208\n",
      "iteration 58, loss: 0.01984\n",
      "iteration 59, loss: 0.03982\n",
      "iteration 60, loss: 0.01051\n",
      "iteration 61, loss: 0.01338\n",
      "real: 1.137, predicted: 1.114\n",
      "iteration 62, loss: 0.00909\n",
      "iteration 63, loss: 0.00743\n",
      "iteration 64, loss: 0.04298\n",
      "iteration 65, loss: 0.01485\n",
      "iteration 66, loss: 0.00723\n",
      "iteration 67, loss: 0.00909\n",
      "iteration 68, loss: 0.01674\n",
      "iteration 69, loss: 0.00478\n",
      "iteration 70, loss: 0.00463\n",
      "iteration 71, loss: 0.01493\n",
      "real: 12.909, predicted: 12.794\n",
      "iteration 72, loss: 0.00261\n",
      "iteration 73, loss: 0.00270\n",
      "iteration 74, loss: 0.02435\n",
      "iteration 75, loss: 0.00954\n",
      "iteration 76, loss: 0.03104\n",
      "iteration 77, loss: 0.03009\n",
      "iteration 78, loss: 0.00488\n",
      "iteration 79, loss: 0.01429\n",
      "iteration 80, loss: 0.00029\n"
     ]
    }
   ],
   "source": [
    "model = LinearRegression(ndim=20, l2_weight=0.01)\n",
    "X = np.random.randn(2, 20)\n",
    "print(\"model.predict:\", model.predict(X))\n",
    "\n",
    "# train the model to predict sum(x).\n",
    "for it in range(80):\n",
    "    X = np.random.randn(10, 20)\n",
    "    y = np.sum(X, axis=1)\n",
    "    loss = model.train(X, y, 0.01)\n",
    "    print(\"iteration %d, loss: %.5f\" % (it + 1, loss))\n",
    "    if it % 10 == 0:\n",
    "        test_X = np.random.randn(1, 20)\n",
    "        test_y = np.sum(test_X, axis=1)\n",
    "        pred_y = model.predict(test_X)\n",
    "        print(\"real: %.3f, predicted: %.3f\" % (test_y, pred_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
