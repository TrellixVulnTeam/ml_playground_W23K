{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Perceptron (MLP)\n",
    "Multilayer Perceptron (MLP) is probably the simplist form of neuroal networks. It is mainly used for classification problem where there is stron non-linearality among features. A MLP consists of an input layer, one or more hidden layers (including the output layer), each of which is a full connected layer. The output of each layer goes through an activation function (usually ReLu) before going to the next layer.\n",
    "\n",
    "![MLP](./imgs/multilayer_perceptron.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MultiLayerPerceptron:\n",
    "    def __init__(self, ndim, hidden_layer_dims):\n",
    "        all_dims = [ndim] + hidden_layer_dims\n",
    "        self.Ws = [np.random.randn(all_dims[i] + 1, all_dims[i + 1]) for i in range(len(all_dims) - 1)]\n",
    "        \n",
    "    def relu_(self, x):\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def relu_prime_(self, x):\n",
    "        def func(v):\n",
    "            return 1 if v > 0 else 0\n",
    "        vfunc = np.vectorize(func)\n",
    "        return vfunc(x)\n",
    "    \n",
    "    def cost_(self, y, hat_y):\n",
    "        return 0.5 * (y - hat_y)**2\n",
    "    \n",
    "    def cost_prime_(self, y, hat_y):\n",
    "        return (y - hat_y)\n",
    "    \n",
    "    def predict_(self, X):\n",
    "        \"\"\"Conduct prediction for a batch of inputs.\"\"\"\n",
    "        bias = np.ones((X.shape[0], 1))\n",
    "        hiddens_wo_activation = []  # store the intermediate values for the hidden layers before activation\n",
    "        outputs = np.concatenate((bias, X), axis=1)\n",
    "        for W in self.Ws[:-1]:\n",
    "            outputs = outputs.dot(W)\n",
    "            outputs = np.concatenate((np.ones((outputs.shape[0], 1)), outputs), axis=1)  # prepend bias\n",
    "            hiddens_wo_activation.append(outputs)\n",
    "            outputs = self.relu_(outputs)\n",
    "        # apply the last layer without relu\n",
    "        outputs = outputs.dot(self.Ws[-1])  # dim: (n_batch, n_out)\n",
    "        hiddens_wo_activation.append(outputs)\n",
    "#         for h in hiddens_wo_activation:\n",
    "#             print(h.shape)\n",
    "        return outputs, hiddens_wo_activation\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.predict_(X)[0]\n",
    "    \n",
    "    def train(self, X, y, lr=0.01):\n",
    "        pred_y, hiddens_wo_activation = self.predict_(X)\n",
    "        # backpropagation with chain rule\n",
    "        # Last layer: E_out = C'(Wn) = cost' * relu'(Hn)\n",
    "        # Other layers: E_n = E_n+1 * W_n * relu'(Zn)\n",
    "        error_by_layers = []\n",
    "#         print(\"Output layer:\", self.cost_prime_(y, pred_y).shape, self.relu_prime_(hiddens_wo_activation[-1]).shape)\n",
    "        error_outputs = self.cost_prime_(y, pred_y) * self.relu_prime_(hiddens_wo_activation[-1])  # dim: (n_batch, n_out)\n",
    "        error_by_layers.append(error_outputs)\n",
    "        for layer in range(len(self.Ws) - 1, 0, -1):  # from later layers to earlier layers except the first layer\n",
    "#             batchW = np.stack([self.Ws[layer]] * X.shape[0])\n",
    "            print(\n",
    "                \"layer: \", layer, \n",
    "                \"error[%d].shape:\" % (len(error_by_layers) - 1), np.sum(error_by_layers[-1], axis=0).shape, \n",
    "                \", batchW[%d]: \" % layer, self.Ws[layer].shape, \n",
    "                \", relu' hidden_wo_activation[%d]\" % (layer - 1), self.relu_prime_(hiddens_wo_activation[layer-1]).shape)\n",
    "            error = np.expand_dims(np.sum(error_by_layers[-1], axis=0), axis=1).T.dot(self.Ws[layer].T)  # dim: (1, h_n)\n",
    "            print(\"error.shape:\", error.shape)\n",
    "            relu_prime_z = np.sum(self.relu_prime_(hiddens_wo_activation[layer-1]), axis=0)  # dim: (, h_n)\n",
    "            relu_prime_z = np.expand_dims(relu_prime_z, axis=1)  # dim: (1, h_n)\n",
    "            error = error * relu_prime_z.T  # dim: (h_n, 1)\n",
    "            print(\"error of layer %d: \" % layer, error.shape)\n",
    "            error_by_layers.append(error)\n",
    "        \n",
    "        \n",
    "        return np.abs(error_outputs).sum() / error_outputs.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer:  1 error[0].shape: (2,) , batchW[1]:  (9, 2) , relu' hidden_wo_activation[0] (10, 9)\n",
      "error.shape: (1, 9)\n",
      "error of layer 1:  (1, 9)\n",
      "iteration 0, loss: 3.903\n"
     ]
    }
   ],
   "source": [
    "n_batch = 10\n",
    "n_dim = 15\n",
    "hidden_layer_dims = [8, 2]\n",
    "\n",
    "model = MultiLayerPerceptron(n_dim, hidden_layer_dims)\n",
    "\n",
    "for it in range(1):\n",
    "    X = np.random.randn(n_batch, n_dim)\n",
    "    y = np.array([[1, 0] if r >= 2 else [0, 1] for r in np.sum(X, axis=1)])  # dim: (n_batch, n_out)\n",
    "    loss = model.train(X, y, lr=0.001)\n",
    "    if it % 100 == 0:\n",
    "        print('iteration %d, loss: %.3f' % (it, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
