{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "/home/yxjiang/source/ml_playground\n"
    }
   ],
   "source": [
    "# env setup\n",
    "import os\n",
    "import sys\n",
    "\n",
    "os.chdir(\"/home/yxjiang/source/ml_playground\")\n",
    "\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Destination folder [/tmp/data] exists.\nTarget file [aclImdb_v1.tar.gz] exists, skip downloading.\nStart to extract [/tmp/data/aclImdb_v1.tar.gz] to [/tmp/data]...\nFile extracted\nProcessing vocabulary from [/tmp/data/aclImdb].\nThere size of vocabulary is : 89527\n"
    }
   ],
   "source": [
    "# data downloading\n",
    "from util import data_util\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# dataset_url=\"https://s3.amazonaws.com/fast-ai-nlp/dbpedia_csv.tgz\"\n",
    "dataset_url=\"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "\n",
    "dest_dir = \"/tmp/data\"\n",
    "dataset_folder_path = os.path.join(dest_dir, \"aclImdb\")\n",
    "data_util.download_data(url=dataset_url, dest_dir=dest_dir)\n",
    "\n",
    "# generate word to id mapping\n",
    "word_to_id, word_list = data_util.get_vocabulary(folder_path=dataset_folder_path, file_suffix=\"vocab\")\n",
    "print(\"There size of vocabulary is :\", len(word_to_id))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform, dataset and dataloader\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "class CutOrPadTransform:\n",
    "    \"\"\"\n",
    "    Shape all sentences to the equal length.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "    \n",
    "    def __call__(self, input):\n",
    "        if len(input[\"words\"]) >= config.sentence_max_length:\n",
    "            input[\"words\"] = input[\"words\"][:config.sentence_max_length]\n",
    "        else:\n",
    "            input[\"words\"].extend([\" \"] * (config.sentence_max_length - len(input[\"words\"])))\n",
    "        return input\n",
    "\n",
    "\n",
    "class WordsToIdsTransform:\n",
    "    \"\"\"\n",
    "    Convert the list of words to embeddings.\n",
    "    \"\"\"\n",
    "    def __init__(self, config, word_to_id):\n",
    "        self.config = config\n",
    "        self.word_to_id = word_to_id\n",
    "    \n",
    "    def __call__(self, input):\n",
    "        input[\"word_ids\"] = torch.tensor([self.word_to_id[w.lower()] for w in input[\"words\"]], dtype=torch.long)\n",
    "        # del input['words']\n",
    "        return input\n",
    "\n",
    "\n",
    "class MovieReviewDataset(Dataset):\n",
    "    def __init__(self, config, pos_data_folder, neg_data_folder, word_to_id, transform):\n",
    "        self.config = config\n",
    "        self.word_to_id = word_to_id\n",
    "        self.data = []\n",
    "        # read all data into memory\n",
    "        for filename in os.listdir(pos_data_folder):\n",
    "            if filename.endswith(\".txt\"):\n",
    "                with open(os.path.join(pos_data_folder, filename), \"r\") as f:\n",
    "                    self.data.append((f.readline(), 1))\n",
    "\n",
    "        for filename in os.listdir(neg_data_folder):\n",
    "            if filename.endswith(\".txt\"):\n",
    "                with open(os.path.join(neg_data_folder, filename), \"r\") as f:\n",
    "                    self.data.append((f.readline(), 0))\n",
    "\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        words = [w.strip() for w in self.data[idx][0].strip().split(\" \")]\n",
    "        label = self.data[idx][1]\n",
    "        input = self.transform({\"words\": words, \"label\": label})\n",
    "        # print(input[\"words\"], \"\\n\", input[\"word_ids\"], \"\\n\", input[\"label\"])\n",
    "        return input[\"words\"], input[\"word_ids\"], input[\"label\"]\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, config, vocabulary_size):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embed = nn.Embedding(vocabulary_size, config.word_embedding_length)\n",
    "        self.hidden_size = 128\n",
    "        self.num_layers = 1\n",
    "        self.directions = 1\n",
    "        self.rnn = nn.RNN(input_size=config.word_embedding_length, hidden_size=self.hidden_size, num_layers=self.num_layers)\n",
    "        self.fc1 = nn.Linear(in_features=self.hidden_size, out_features=64)\n",
    "        self.fc2 = nn.Linear(in_features=64, out_features=config.num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch = x.shape[0]\n",
    "        x = self.embed(x)  # (batch, sentence_length, embedding_dim)\n",
    "        x = x.permute(1, 0, 2).contiguous()  # (sentence_length, batch, embedding_dim)\n",
    "\n",
    "        h0 = torch.zeros((self.num_layers * self.directions, batch, self.hidden_size)).to(device)\n",
    "        output, ht = self.rnn(x, h0)\n",
    "        ht = ht.permute(1, 0, 2)  # (batch, num_layer * directions, embedding_dim)\n",
    "        ht = ht.contiguous().view(batch, self.num_layers * self.directions, self.hidden_size)\n",
    "        x = F.relu(self.fc1(ht))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = x.view(batch, -1)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, config, vocabulary_size):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embed = nn.Embedding(vocabulary_size, config.word_embedding_length)\n",
    "        self.conv_layer_sizes = config.conv_layer_sizes\n",
    "\n",
    "        for i, size in enumerate(self.conv_layer_sizes):\n",
    "            self.add_module(\"conv\" + str(i), nn.Conv2d(1, 1, kernel_size=(size, config.word_embedding_length)).to(device))\n",
    "            self.add_module(\"pool\" + str(i), nn.MaxPool2d((config.sentence_max_length - size + 1, 1)).to(device))\n",
    "\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        self.fc = nn.Linear(len(self.conv_layer_sizes), config.num_classes)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch = x.shape[0]\n",
    "        x = torch.unsqueeze(self.embed(x), 1)  # [NCHW], add channel to dimension 1\n",
    "        # convs\n",
    "        xs = []\n",
    "        for i in range(len(self.conv_layer_sizes)):\n",
    "            xs.append(self.config.activation(self._modules[\"conv\" + str(i)](x)))  # conv modules\n",
    "            xs[i] = self._modules[\"pool\" + str(i)](xs[i])  # max over time pooling modules\n",
    "\n",
    "        x = torch.cat(xs, dim=-1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        x = x.view(batch, -1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "def train(model, config, train_dataloader, test_dataloader, device, check_interval=5000):\n",
    "    criteria = config.criteria()\n",
    "    optimizer = config.optimizer(model.parameters(), config.lr)\n",
    "    start = time.time()\n",
    "    counts = 0\n",
    "    writer = SummaryWriter(filename_suffix=str(config))\n",
    "    # writer.add_graph(model)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    for epoch in range(config.epochs):\n",
    "        for i, (words, word_ids, labels) in enumerate(train_dataloader):\n",
    "            counts += labels.shape[0]\n",
    "            optimizer.zero_grad()\n",
    "            output = model(word_ids.to(device))\n",
    "            loss = criteria(output, labels.to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if ((epoch + 1) * i) % check_interval == 0:\n",
    "                print(\"[%d seconds](epoch: %d/%d)[%d samples] loss: %.3f.\" % (time.time() - start, epoch + 1, config.epochs, counts, loss.mean().item()))\n",
    "                # eval on test dataset\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    acc_eval_loss = 0.0\n",
    "                    batches = 0\n",
    "                    correct = 0\n",
    "                    total_samples = 0\n",
    "                    for j, (words, eval_word_ids, eval_labels) in enumerate(test_dataloader):\n",
    "                        eval_output = model(eval_word_ids.to(device))\n",
    "                        eval_loss = criteria(eval_output, eval_labels.to(device))\n",
    "                        acc_eval_loss += eval_loss.item()\n",
    "                        batches += 1\n",
    "                        correct += torch.sum(torch.argmax(eval_output.cpu(), dim=1) == eval_labels)\n",
    "                        total_samples += eval_word_ids.shape[0]\n",
    "                    accuracy = 100.0 * correct / total_samples\n",
    "                    print('eval loss: %.3f, accuracy: %.3f%% [%d/%d]' % (acc_eval_loss / batches, accuracy, correct, total_samples))\n",
    "                    writer.add_scalar('Loss/eval', acc_eval_loss / batches, epoch + 1)\n",
    "                    writer.add_scalar('Eval accuracy', accuracy, epoch + 1)\n",
    "                model.train()\n",
    "                writer.add_scalar('Loss/train', loss.mean().item(), epoch + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configs\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.num_classes = 2\n",
    "        self.sentence_max_length = 40\n",
    "        self.word_embedding_length = 128\n",
    "        self.activation = F.relu\n",
    "        self.criteria = nn.CrossEntropyLoss\n",
    "        self.optimizer = optim.Adam\n",
    "        self.lr = 0.0001\n",
    "        self.epochs = 500\n",
    "        self.batch_size = 1024\n",
    "        self.dropout = 0.1\n",
    "        self.conv_layer_sizes = [3, 4, 5, 6, 7, 8]\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"sentence_max_len_%d-embedding-%d-lr-%.8f-batch_size-%d-dropout-%.2f-conv_layers-%s\" % (self.sentence_max_length, self.word_embedding_length, self.lr, self.batch_size, self.dropout, \"|\".join([str(s) for s in self.conv_layer_sizes]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": " 0.682.\neval loss: 0.689, accuracy: 53.560% [13390/25000]\n[35 seconds](epoch: 11/500)[251024 samples] loss: 0.679.\neval loss: 0.688, accuracy: 53.980% [13495/25000]\n[39 seconds](epoch: 12/500)[276024 samples] loss: 0.682.\neval loss: 0.686, accuracy: 54.268% [13567/25000]\n[42 seconds](epoch: 13/500)[301024 samples] loss: 0.676.\neval loss: 0.685, accuracy: 54.716% [13679/25000]\n[46 seconds](epoch: 14/500)[326024 samples] loss: 0.676.\neval loss: 0.683, accuracy: 55.028% [13757/25000]\n[49 seconds](epoch: 15/500)[351024 samples] loss: 0.670.\neval loss: 0.682, accuracy: 55.500% [13875/25000]\n[53 seconds](epoch: 16/500)[376024 samples] loss: 0.678.\neval loss: 0.680, accuracy: 55.684% [13921/25000]\n[56 seconds](epoch: 17/500)[401024 samples] loss: 0.664.\neval loss: 0.679, accuracy: 55.928% [13982/25000]\n[60 seconds](epoch: 18/500)[426024 samples] loss: 0.673.\neval loss: 0.678, accuracy: 56.216% [14054/25000]\n[63 seconds](epoch: 19/500)[451024 samples] loss: 0.667.\neval loss: 0.676, accuracy: 56.620% [14155/25000]\n[67 seconds](epoch: 20/500)[476024 samples] loss: 0.661.\neval loss: 0.675, accuracy: 56.856% [14214/25000]\n[70 seconds](epoch: 21/500)[501024 samples] loss: 0.667.\neval loss: 0.674, accuracy: 56.996% [14249/25000]\n[74 seconds](epoch: 22/500)[526024 samples] loss: 0.658.\neval loss: 0.673, accuracy: 57.136% [14284/25000]\n[78 seconds](epoch: 23/500)[551024 samples] loss: 0.663.\neval loss: 0.671, accuracy: 57.256% [14314/25000]\n[81 seconds](epoch: 24/500)[576024 samples] loss: 0.665.\neval loss: 0.670, accuracy: 57.480% [14370/25000]\n[84 seconds](epoch: 25/500)[601024 samples] loss: 0.654.\neval loss: 0.669, accuracy: 57.632% [14408/25000]\n[88 seconds](epoch: 26/500)[626024 samples] loss: 0.662.\neval loss: 0.668, accuracy: 57.784% [14446/25000]\n[92 seconds](epoch: 27/500)[651024 samples] loss: 0.654.\neval loss: 0.667, accuracy: 57.968% [14492/25000]\n[95 seconds](epoch: 28/500)[676024 samples] loss: 0.655.\neval loss: 0.667, accuracy: 58.180% [14545/25000]\n[99 seconds](epoch: 29/500)[701024 samples] loss: 0.651.\neval loss: 0.666, accuracy: 58.208% [14552/25000]\n[102 seconds](epoch: 30/500)[726024 samples] loss: 0.652.\neval loss: 0.665, accuracy: 58.292% [14573/25000]\n[106 seconds](epoch: 31/500)[751024 samples] loss: 0.648.\neval loss: 0.664, accuracy: 58.364% [14591/25000]\n[109 seconds](epoch: 32/500)[776024 samples] loss: 0.641.\neval loss: 0.663, accuracy: 58.528% [14632/25000]\n[113 seconds](epoch: 33/500)[801024 samples] loss: 0.637.\neval loss: 0.662, accuracy: 58.620% [14655/25000]\n[116 seconds](epoch: 34/500)[826024 samples] loss: 0.649.\neval loss: 0.662, accuracy: 58.716% [14679/25000]\n[120 seconds](epoch: 35/500)[851024 samples] loss: 0.645.\neval loss: 0.661, accuracy: 58.760% [14690/25000]\n[123 seconds](epoch: 36/500)[876024 samples] loss: 0.639.\neval loss: 0.660, accuracy: 58.944% [14736/25000]\n[127 seconds](epoch: 37/500)[901024 samples] loss: 0.619.\neval loss: 0.660, accuracy: 59.104% [14776/25000]\n[130 seconds](epoch: 38/500)[926024 samples] loss: 0.629.\neval loss: 0.659, accuracy: 59.140% [14785/25000]\n[134 seconds](epoch: 39/500)[951024 samples] loss: 0.624.\neval loss: 0.658, accuracy: 59.216% [14804/25000]\n[137 seconds](epoch: 40/500)[976024 samples] loss: 0.628.\neval loss: 0.657, accuracy: 59.280% [14820/25000]\n[141 seconds](epoch: 41/500)[1001024 samples] loss: 0.630.\neval loss: 0.657, accuracy: 59.372% [14843/25000]\n[144 seconds](epoch: 42/500)[1026024 samples] loss: 0.630.\neval loss: 0.656, accuracy: 59.472% [14868/25000]\n[148 seconds](epoch: 43/500)[1051024 samples] loss: 0.627.\neval loss: 0.655, accuracy: 59.592% [14898/25000]\n[151 seconds](epoch: 44/500)[1076024 samples] loss: 0.627.\neval loss: 0.655, accuracy: 59.656% [14914/25000]\n[155 seconds](epoch: 45/500)[1101024 samples] loss: 0.637.\neval loss: 0.654, accuracy: 59.696% [14924/25000]\n[158 seconds](epoch: 46/500)[1126024 samples] loss: 0.626.\neval loss: 0.654, accuracy: 59.804% [14951/25000]\n[162 seconds](epoch: 47/500)[1151024 samples] loss: 0.612.\neval loss: 0.653, accuracy: 59.876% [14969/25000]\n[165 seconds](epoch: 48/500)[1176024 samples] loss: 0.622.\neval loss: 0.652, accuracy: 59.872% [14968/25000]\n[169 seconds](epoch: 49/500)[1201024 samples] loss: 0.612.\neval loss: 0.651, accuracy: 59.900% [14975/25000]\n[172 seconds](epoch: 50/500)[1226024 samples] loss: 0.606.\neval loss: 0.651, accuracy: 59.956% [14989/25000]\n[176 seconds](epoch: 51/500)[1251024 samples] loss: 0.620.\neval loss: 0.650, accuracy: 60.128% [15032/25000]\n[180 seconds](epoch: 52/500)[1276024 samples] loss: 0.610.\neval loss: 0.650, accuracy: 60.204% [15051/25000]\n[183 seconds](epoch: 53/500)[1301024 samples] loss: 0.598.\neval loss: 0.649, accuracy: 60.280% [15070/25000]\n[186 seconds](epoch: 54/500)[1326024 samples] loss: 0.596.\neval loss: 0.648, accuracy: 60.348% [15087/25000]\n[190 seconds](epoch: 55/500)[1351024 samples] loss: 0.598.\neval loss: 0.648, accuracy: 60.460% [15115/25000]\n[194 seconds](epoch: 56/500)[1376024 samples] loss: 0.606.\neval loss: 0.647, accuracy: 60.508% [15127/25000]\n[197 seconds](epoch: 57/500)[1401024 samples] loss: 0.603.\neval loss: 0.647, accuracy: 60.544% [15136/25000]\n[201 seconds](epoch: 58/500)[1426024 samples] loss: 0.590.\neval loss: 0.646, accuracy: 60.592% [15148/25000]\n[204 seconds](epoch: 59/500)[1451024 samples] loss: 0.613.\neval loss: 0.646, accuracy: 60.668% [15167/25000]\n[208 seconds](epoch: 60/500)[1476024 samples] loss: 0.598.\neval loss: 0.645, accuracy: 60.804% [15201/25000]\n[211 seconds](epoch: 61/500)[1501024 samples] loss: 0.585.\neval loss: 0.645, accuracy: 60.984% [15246/25000]\n[215 seconds](epoch: 62/500)[1526024 samples] loss: 0.580.\neval loss: 0.644, accuracy: 61.064% [15266/25000]\n[219 seconds](epoch: 63/500)[1551024 samples] loss: 0.589.\neval loss: 0.643, accuracy: 61.180% [15295/25000]\n[222 seconds](epoch: 64/500)[1576024 samples] loss: 0.593.\neval loss: 0.643, accuracy: 61.220% [15305/25000]\n[225 seconds](epoch: 65/500)[1601024 samples] loss: 0.579.\neval loss: 0.642, accuracy: 61.356% [15339/25000]\n[229 seconds](epoch: 66/500)[1626024 samples] loss: 0.569.\neval loss: 0.642, accuracy: 61.508% [15377/25000]\n[233 seconds](epoch: 67/500)[1651024 samples] loss: 0.570.\neval loss: 0.641, accuracy: 61.524% [15381/25000]\n[236 seconds](epoch: 68/500)[1676024 samples] loss: 0.590.\neval loss: 0.641, accuracy: 61.628% [15407/25000]\n[240 seconds](epoch: 69/500)[1701024 samples] loss: 0.574.\neval loss: 0.640, accuracy: 61.580% [15395/25000]\n[243 seconds](epoch: 70/500)[1726024 samples] loss: 0.567.\neval loss: 0.640, accuracy: 61.632% [15408/25000]\n[247 seconds](epoch: 71/500)[1751024 samples] loss: 0.550.\neval loss: 0.639, accuracy: 61.708% [15427/25000]\n[250 seconds](epoch: 72/500)[1776024 samples] loss: 0.564.\neval loss: 0.639, accuracy: 61.704% [15426/25000]\n[254 seconds](epoch: 73/500)[1801024 samples] loss: 0.556.\neval loss: 0.638, accuracy: 61.776% [15444/25000]\n[257 seconds](epoch: 74/500)[1826024 samples] loss: 0.554.\neval loss: 0.638, accuracy: 61.880% [15470/25000]\n[261 seconds](epoch: 75/500)[1851024 samples] loss: 0.566.\neval loss: 0.637, accuracy: 61.908% [15477/25000]\n[264 seconds](epoch: 76/500)[1876024 samples] loss: 0.545.\neval loss: 0.637, accuracy: 61.940% [15485/25000]\n[268 seconds](epoch: 77/500)[1901024 samples] loss: 0.540.\neval loss: 0.637, accuracy: 62.064% [15516/25000]\n[271 seconds](epoch: 78/500)[1926024 samples] loss: 0.555.\neval loss: 0.637, accuracy: 62.192% [15548/25000]\n[275 seconds](epoch: 79/500)[1951024 samples] loss: 0.540.\neval loss: 0.636, accuracy: 62.232% [15558/25000]\n[278 seconds](epoch: 80/500)[1976024 samples] loss: 0.538.\neval loss: 0.636, accuracy: 62.324% [15581/25000]\n[282 seconds](epoch: 81/500)[2001024 samples] loss: 0.545.\neval loss: 0.636, accuracy: 62.376% [15594/25000]\n[286 seconds](epoch: 82/500)[2026024 samples] loss: 0.547.\neval loss: 0.635, accuracy: 62.440% [15610/25000]\n[289 seconds](epoch: 83/500)[2051024 samples] loss: 0.537.\neval loss: 0.635, accuracy: 62.520% [15630/25000]\n[292 seconds](epoch: 84/500)[2076024 samples] loss: 0.527.\neval loss: 0.635, accuracy: 62.452% [15613/25000]\n[296 seconds](epoch: 85/500)[2101024 samples] loss: 0.548.\neval loss: 0.635, accuracy: 62.572% [15643/25000]\n[300 seconds](epoch: 86/500)[2126024 samples] loss: 0.534.\neval loss: 0.634, accuracy: 62.632% [15658/25000]\n[303 seconds](epoch: 87/500)[2151024 samples] loss: 0.547.\neval loss: 0.634, accuracy: 62.672% [15668/25000]\n[307 seconds](epoch: 88/500)[2176024 samples] loss: 0.514.\neval loss: 0.634, accuracy: 62.760% [15690/25000]\n[310 seconds](epoch: 89/500)[2201024 samples] loss: 0.537.\neval loss: 0.634, accuracy: 62.828% [15707/25000]\n[314 seconds](epoch: 90/500)[2226024 samples] loss: 0.502.\neval loss: 0.634, accuracy: 62.852% [15713/25000]\n[317 seconds](epoch: 91/500)[2251024 samples] loss: 0.524.\neval loss: 0.634, accuracy: 62.932% [15733/25000]\n[321 seconds](epoch: 92/500)[2276024 samples] loss: 0.520.\neval loss: 0.634, accuracy: 62.956% [15739/25000]\n[324 seconds](epoch: 93/500)[2301024 samples] loss: 0.523.\neval loss: 0.634, accuracy: 63.068% [15767/25000]\n[328 seconds](epoch: 94/500)[2326024 samples] loss: 0.515.\neval loss: 0.634, accuracy: 63.096% [15774/25000]\n[331 seconds](epoch: 95/500)[2351024 samples] loss: 0.513.\neval loss: 0.634, accuracy: 63.164% [15791/25000]\n[335 seconds](epoch: 96/500)[2376024 samples] loss: 0.528.\neval loss: 0.634, accuracy: 63.208% [15802/25000]\n[339 seconds](epoch: 97/500)[2401024 samples] loss: 0.499.\neval loss: 0.634, accuracy: 63.276% [15819/25000]\n[342 seconds](epoch: 98/500)[2426024 samples] loss: 0.498.\neval loss: 0.634, accuracy: 63.320% [15830/25000]\n[345 seconds](epoch: 99/500)[2451024 samples] loss: 0.498.\neval loss: 0.634, accuracy: 63.424% [15856/25000]\n[349 seconds](epoch: 100/500)[2476024 samples] loss: 0.510.\neval loss: 0.634, accuracy: 63.420% [15855/25000]\n[353 seconds](epoch: 101/500)[2501024 samples] loss: 0.464.\neval loss: 0.634, accuracy: 63.484% [15871/25000]\n[356 seconds](epoch: 102/500)[2526024 samples] loss: 0.484.\neval loss: 0.634, accuracy: 63.532% [15883/25000]\n[360 seconds](epoch: 103/500)[2551024 samples] loss: 0.474.\neval loss: 0.634, accuracy: 63.528% [15882/25000]\n[363 seconds](epoch: 104/500)[2576024 samples] loss: 0.487.\neval loss: 0.634, accuracy: 63.596% [15899/25000]\n[367 seconds](epoch: 105/500)[2601024 samples] loss: 0.487.\neval loss: 0.634, accuracy: 63.644% [15911/25000]\n[370 seconds](epoch: 106/500)[2626024 samples] loss: 0.472.\neval loss: 0.634, accuracy: 63.604% [15901/25000]\n[374 seconds](epoch: 107/500)[2651024 samples] loss: 0.482.\neval loss: 0.634, accuracy: 63.600% [15900/25000]\n[377 seconds](epoch: 108/500)[2676024 samples] loss: 0.480.\neval loss: 0.634, accuracy: 63.688% [15922/25000]\n[381 seconds](epoch: 109/500)[2701024 samples] loss: 0.451.\neval loss: 0.634, accuracy: 63.728% [15932/25000]\n[384 seconds](epoch: 110/500)[2726024 samples] loss: 0.484.\neval loss: 0.635, accuracy: 63.772% [15943/25000]\n[388 seconds](epoch: 111/500)[2751024 samples] loss: 0.483.\neval loss: 0.635, accuracy: 63.832% [15958/25000]\n[391 seconds](epoch: 112/500)[2776024 samples] loss: 0.491.\neval loss: 0.635, accuracy: 63.856% [15964/25000]\n[395 seconds](epoch: 113/500)[2801024 samples] loss: 0.455.\neval loss: 0.635, accuracy: 63.888% [15972/25000]\n[398 seconds](epoch: 114/500)[2826024 samples] loss: 0.478.\neval loss: 0.635, accuracy: 63.916% [15979/25000]\n[402 seconds](epoch: 115/500)[2851024 samples] loss: 0.479.\neval loss: 0.636, accuracy: 63.912% [15978/25000]\n[406 seconds](epoch: 116/500)[2876024 samples] loss: 0.457.\neval loss: 0.636, accuracy: 63.896% [15974/25000]\n[409 seconds](epoch: 117/500)[2901024 samples] loss: 0.475.\neval loss: 0.636, accuracy: 63.984% [15996/25000]\n[413 seconds](epoch: 118/500)[2926024 samples] loss: 0.461.\neval loss: 0.637, accuracy: 63.996% [15999/25000]\n[416 seconds](epoch: 119/500)[2951024 samples] loss: 0.451.\neval loss: 0.637, accuracy: 64.080% [16020/25000]\n[420 seconds](epoch: 120/500)[2976024 samples] loss: 0.459.\neval loss: 0.637, accuracy: 64.096% [16024/25000]\n[423 seconds](epoch: 121/500)[3001024 samples] loss: 0.438.\neval loss: 0.637, accuracy: 64.092% [16023/25000]\n[427 seconds](epoch: 122/500)[3026024 samples] loss: 0.451.\neval loss: 0.638, accuracy: 64.104% [16026/25000]\n[430 seconds](epoch: 123/500)[3051024 samples] loss: 0.457.\neval loss: 0.638, accuracy: 64.100% [16025/25000]\n[434 seconds](epoch: 124/500)[3076024 samples] loss: 0.449.\neval loss: 0.638, accuracy: 64.188% [16047/25000]\n[437 seconds](epoch: 125/500)[3101024 samples] loss: 0.427.\neval loss: 0.639, accuracy: 64.200% [16050/25000]\n[441 seconds](epoch: 126/500)[3126024 samples] loss: 0.449.\neval loss: 0.639, accuracy: 64.200% [16050/25000]\n[444 seconds](epoch: 127/500)[3151024 samples] loss: 0.425.\neval loss: 0.640, accuracy: 64.252% [16063/25000]\n[448 seconds](epoch: 128/500)[3176024 samples] loss: 0.440.\neval loss: 0.640, accuracy: 64.244% [16061/25000]\n[451 seconds](epoch: 129/500)[3201024 samples] loss: 0.424.\neval loss: 0.640, accuracy: 64.276% [16069/25000]\n[455 seconds](epoch: 130/500)[3226024 samples] loss: 0.463.\neval loss: 0.641, accuracy: 64.264% [16066/25000]\n[458 seconds](epoch: 131/500)[3251024 samples] loss: 0.418.\neval loss: 0.641, accuracy: 64.296% [16074/25000]\n[462 seconds](epoch: 132/500)[3276024 samples] loss: 0.402.\neval loss: 0.642, accuracy: 64.336% [16084/25000]\n[466 seconds](epoch: 133/500)[3301024 samples] loss: 0.433.\neval loss: 0.642, accuracy: 64.396% [16099/25000]\n[469 seconds](epoch: 134/500)[3326024 samples] loss: 0.424.\neval loss: 0.643, accuracy: 64.384% [16096/25000]\n[473 seconds](epoch: 135/500)[3351024 samples] loss: 0.436.\neval loss: 0.643, accuracy: 64.428% [16107/25000]\n[476 seconds](epoch: 136/500)[3376024 samples] loss: 0.415.\neval loss: 0.644, accuracy: 64.452% [16113/25000]\n[480 seconds](epoch: 137/500)[3401024 samples] loss: 0.431.\neval loss: 0.644, accuracy: 64.388% [16097/25000]\n[483 seconds](epoch: 138/500)[3426024 samples] loss: 0.417.\neval loss: 0.645, accuracy: 64.480% [16120/25000]\n[487 seconds](epoch: 139/500)[3451024 samples] loss: 0.392.\neval loss: 0.645, accuracy: 64.484% [16121/25000]\n[490 seconds](epoch: 140/500)[3476024 samples] loss: 0.390.\neval loss: 0.646, accuracy: 64.496% [16124/25000]\n[494 seconds](epoch: 141/500)[3501024 samples] loss: 0.404.\neval loss: 0.646, accuracy: 64.532% [16133/25000]\n[497 seconds](epoch: 142/500)[3526024 samples] loss: 0.427.\neval loss: 0.647, accuracy: 64.536% [16134/25000]\n[501 seconds](epoch: 143/500)[3551024 samples] loss: 0.412.\neval loss: 0.647, accuracy: 64.520% [16130/25000]\n[504 seconds](epoch: 144/500)[3576024 samples] loss: 0.410.\neval loss: 0.648, accuracy: 64.556% [16139/25000]\n[508 seconds](epoch: 145/500)[3601024 samples] loss: 0.407.\neval loss: 0.649, accuracy: 64.540% [16135/25000]\n[512 seconds](epoch: 146/500)[3626024 samples] loss: 0.407.\neval loss: 0.649, accuracy: 64.588% [16147/25000]\n[515 seconds](epoch: 147/500)[3651024 samples] loss: 0.398.\neval loss: 0.650, accuracy: 64.560% [16140/25000]\n[519 seconds](epoch: 148/500)[3676024 samples] loss: 0.393.\neval loss: 0.651, accuracy: 64.552% [16138/25000]\n[522 seconds](epoch: 149/500)[3701024 samples] loss: 0.406.\neval loss: 0.651, accuracy: 64.608% [16152/25000]\n[526 seconds](epoch: 150/500)[3726024 samples] loss: 0.396.\neval loss: 0.652, accuracy: 64.640% [16160/25000]\n[529 seconds](epoch: 151/500)[3751024 samples] loss: 0.402.\neval loss: 0.652, accuracy: 64.676% [16169/25000]\n[533 seconds](epoch: 152/500)[3776024 samples] loss: 0.410.\neval loss: 0.653, accuracy: 64.664% [16166/25000]\n[536 seconds](epoch: 153/500)[3801024 samples] loss: 0.383.\neval loss: 0.653, accuracy: 64.692% [16173/25000]\n[540 seconds](epoch: 154/500)[3826024 samples] loss: 0.407.\neval loss: 0.654, accuracy: 64.696% [16174/25000]\n[543 seconds](epoch: 155/500)[3851024 samples] loss: 0.399.\neval loss: 0.655, accuracy: 64.732% [16183/25000]\n[547 seconds](epoch: 156/500)[3876024 samples] loss: 0.377.\neval loss: 0.656, accuracy: 64.768% [16192/25000]\n[551 seconds](epoch: 157/500)[3901024 samples] loss: 0.396.\neval loss: 0.656, accuracy: 64.760% [16190/25000]\n[554 seconds](epoch: 158/500)[3926024 samples] loss: 0.382.\neval loss: 0.657, accuracy: 64.728% [16182/25000]\n[558 seconds](epoch: 159/500)[3951024 samples] loss: 0.377.\neval loss: 0.658, accuracy: 64.788% [16197/25000]\n[561 seconds](epoch: 160/500)[3976024 samples] loss: 0.393.\neval loss: 0.659, accuracy: 64.792% [16198/25000]\n[565 seconds](epoch: 161/500)[4001024 samples] loss: 0.392.\neval loss: 0.659, accuracy: 64.772% [16193/25000]\n[568 seconds](epoch: 162/500)[4026024 samples] loss: 0.366.\neval loss: 0.660, accuracy: 64.832% [16208/25000]\n[572 seconds](epoch: 163/500)[4051024 samples] loss: 0.357.\neval loss: 0.661, accuracy: 64.848% [16212/25000]\n[575 seconds](epoch: 164/500)[4076024 samples] loss: 0.348.\neval loss: 0.661, accuracy: 64.888% [16222/25000]\n[579 seconds](epoch: 165/500)[4101024 samples] loss: 0.378.\neval loss: 0.662, accuracy: 64.808% [16202/25000]\n[582 seconds](epoch: 166/500)[4126024 samples] loss: 0.371.\neval loss: 0.663, accuracy: 64.860% [16215/25000]\n[586 seconds](epoch: 167/500)[4151024 samples] loss: 0.383.\neval loss: 0.664, accuracy: 64.900% [16225/25000]\n[589 seconds](epoch: 168/500)[4176024 samples] loss: 0.367.\neval loss: 0.665, accuracy: 64.848% [16212/25000]\n[593 seconds](epoch: 169/500)[4201024 samples] loss: 0.372.\neval loss: 0.666, accuracy: 64.868% [16217/25000]\n[596 seconds](epoch: 170/500)[4226024 samples] loss: 0.364.\neval loss: 0.666, accuracy: 64.884% [16221/25000]\n[600 seconds](epoch: 171/500)[4251024 samples] loss: 0.342.\neval loss: 0.667, accuracy: 64.908% [16227/25000]\n[603 seconds](epoch: 172/500)[4276024 samples] loss: 0.335.\neval loss: 0.668, accuracy: 64.904% [16226/25000]\n[607 seconds](epoch: 173/500)[4301024 samples] loss: 0.344.\neval loss: 0.669, accuracy: 64.904% [16226/25000]\n[610 seconds](epoch: 174/500)[4326024 samples] loss: 0.342.\neval loss: 0.670, accuracy: 64.952% [16238/25000]\n[614 seconds](epoch: 175/500)[4351024 samples] loss: 0.336.\neval loss: 0.670, accuracy: 64.944% [16236/25000]\n[617 seconds](epoch: 176/500)[4376024 samples] loss: 0.361.\neval loss: 0.671, accuracy: 64.912% [16228/25000]\n[621 seconds](epoch: 177/500)[4401024 samples] loss: 0.363.\neval loss: 0.672, accuracy: 64.920% [16230/25000]\n[625 seconds](epoch: 178/500)[4426024 samples] loss: 0.336.\neval loss: 0.673, accuracy: 64.948% [16237/25000]\n[628 seconds](epoch: 179/500)[4451024 samples] loss: 0.337.\neval loss: 0.674, accuracy: 64.928% [16232/25000]\n[632 seconds](epoch: 180/500)[4476024 samples] loss: 0.359.\neval loss: 0.675, accuracy: 64.944% [16236/25000]\n[635 seconds](epoch: 181/500)[4501024 samples] loss: 0.339.\neval loss: 0.676, accuracy: 64.928% [16232/25000]\n[639 seconds](epoch: 182/500)[4526024 samples] loss: 0.351.\neval loss: 0.677, accuracy: 64.944% [16236/25000]\n[642 seconds](epoch: 183/500)[4551024 samples] loss: 0.341.\neval loss: 0.678, accuracy: 64.936% [16234/25000]\n[646 seconds](epoch: 184/500)[4576024 samples] loss: 0.342.\neval loss: 0.679, accuracy: 64.940% [16235/25000]\n[649 seconds](epoch: 185/500)[4601024 samples] loss: 0.336.\neval loss: 0.680, accuracy: 64.948% [16237/25000]\n[653 seconds](epoch: 186/500)[4626024 samples] loss: 0.331.\neval loss: 0.680, accuracy: 64.936% [16234/25000]\n[656 seconds](epoch: 187/500)[4651024 samples] loss: 0.337.\neval loss: 0.682, accuracy: 64.996% [16249/25000]\n[660 seconds](epoch: 188/500)[4676024 samples] loss: 0.319.\neval loss: 0.682, accuracy: 64.960% [16240/25000]\n[664 seconds](epoch: 189/500)[4701024 samples] loss: 0.353.\neval loss: 0.683, accuracy: 64.984% [16246/25000]\n[667 seconds](epoch: 190/500)[4726024 samples] loss: 0.320.\neval loss: 0.685, accuracy: 65.012% [16253/25000]\n[671 seconds](epoch: 191/500)[4751024 samples] loss: 0.318.\neval loss: 0.685, accuracy: 64.988% [16247/25000]\n[674 seconds](epoch: 192/500)[4776024 samples] loss: 0.306.\neval loss: 0.686, accuracy: 64.996% [16249/25000]\n[678 seconds](epoch: 193/500)[4801024 samples] loss: 0.327.\n"
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-1f5480d9fb57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# model = RNN(config, len(word_list)).to(device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-c549ff16ecaa>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, config, train_dataloader, test_dataloader, device, check_interval)\u001b[0m\n\u001b[1;32m     27\u001b[0m                     \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                     \u001b[0mtotal_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m                     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_word_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_labels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m                         \u001b[0meval_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_word_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m                         \u001b[0meval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriteria\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-2ef5d510b54a>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"words\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"label\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-2ef5d510b54a>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"words\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"label\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# put everything together\n",
    "import time\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "config = Config()\n",
    "\n",
    "pos_train_data_folder = os.path.join(dataset_folder_path, \"train/pos\")\n",
    "neg_train_data_folder = os.path.join(dataset_folder_path, \"train/neg\")\n",
    "train_dataset = MovieReviewDataset(config, pos_train_data_folder, neg_train_data_folder, word_to_id, \n",
    "                            transform=transforms.Compose([\n",
    "                                CutOrPadTransform(config), WordsToIdsTransform(config, word_to_id)\n",
    "                            ]))\n",
    "\n",
    "pos_test_data_folder = os.path.join(dataset_folder_path, \"test/pos\")\n",
    "neg_test_data_folder = os.path.join(dataset_folder_path, \"test/neg\")\n",
    "test_dataset = MovieReviewDataset(config, pos_test_data_folder, neg_test_data_folder, word_to_id, \n",
    "                            transform=transforms.Compose([\n",
    "                                CutOrPadTransform(config), WordsToIdsTransform(config, word_to_id)\n",
    "                            ]))\n",
    "\n",
    "train_dataloader = DataLoader(dataset=train_dataset, batch_size=config.batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(dataset=test_dataset, batch_size=config.batch_size)\n",
    "\n",
    "model = TextCNN(config, len(word_to_id)).to(device)\n",
    "# model = RNN(config, len(word_list)).to(device)\n",
    "\n",
    "train(model, config, train_dataloader, test_dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}