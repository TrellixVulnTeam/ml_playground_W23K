{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "/home/yxjiang/source/ml_playground\n"
    }
   ],
   "source": [
    "# data downloading\n",
    "import data_util\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import sys\n",
    "\n",
    "os.chdir(\"/home/yxjiang/source/ml_playground\")\n",
    "print(os.getcwd())\n",
    "\n",
    "from util import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Destination folder [/tmp/data] exists.\nTarget file [aclImdb_v1.tar.gz] exists, skip downloading.\nStart to extract [/tmp/data/aclImdb_v1.tar.gz] to [/tmp/data]...\nFile extracted\nProcessing vocabulary from [/tmp/data/aclImdb].\nThere size of vocabulary is : 89527\n"
    }
   ],
   "source": [
    "dataset_url=\"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "\n",
    "dest_dir = \"/tmp/data\"\n",
    "dataset_folder_path = os.path.join(dest_dir, \"aclImdb\")\n",
    "data_util.download_data(url=dataset_url, dest_dir=dest_dir)\n",
    "\n",
    "# generate word to id mapping\n",
    "word_to_id, word_list = data_util.get_vocabulary(folder_path=dataset_folder_path, file_suffix=\"vocab\")\n",
    "print(\"There size of vocabulary is :\", len(word_to_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, config, vocabulary_size):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.rnn_type = config.rnn_type\n",
    "        self.embed = nn.Embedding(vocabulary_size, config.word_embedding_length)\n",
    "        self.num_layers = config.num_layers if config.num_layers is not None else 1\n",
    "        self.directions = config.num_directions if config.num_directions is not None else 1\n",
    "        self.hidden_size = 128\n",
    "        if config.rnn_type is nn.RNN:\n",
    "            self.rnn = nn.RNN(\n",
    "                input_size=config.word_embedding_length, \n",
    "                hidden_size=self.hidden_size, \n",
    "                num_layers=self.num_layers,\n",
    "                bidirectional=False if self.directions == 1 else True\n",
    "            )\n",
    "        elif config.rnn_type is nn.LSTM:\n",
    "            self.cell_size = 128\n",
    "            self.rnn = nn.LSTM(\n",
    "                input_size=config.word_embedding_length,\n",
    "                hidden_size=self.hidden_size,\n",
    "                num_layers=self.num_layers,\n",
    "                batch_first=True,\n",
    "                bidirectional=False if self.directions == 1 else True\n",
    "            )\n",
    "        elif config.rnn_type is nn.GRU:\n",
    "            self.rnn = nn.GRU(\n",
    "                input_size=config.word_embedding_length, \n",
    "                hidden_size=self.hidden_size, \n",
    "                num_layers=self.num_layers,\n",
    "                bidirectional=False if self.directions == 1 else True\n",
    "            )\n",
    "        self.fc1 = nn.Linear(in_features=self.hidden_size * self.directions, out_features=64)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        self.fc2 = nn.Linear(in_features=64, out_features=config.num_classes)\n",
    "\n",
    "    def forward(self, x, x_lens):\n",
    "        batch = x.shape[0]\n",
    "        x = self.embed(x)  # (batch, sentence_length, embedding_dim)\n",
    "        x = x.to(self.config.device)\n",
    "        x = pack_padded_sequence(x, x_lens, batch_first=True, enforce_sorted=False)\n",
    "\n",
    "        h0 = torch.zeros((self.num_layers * self.directions, batch, self.hidden_size)).to(self.config.device)\n",
    "        if self.rnn_type is nn.RNN or self.rnn_type is nn.GRU:\n",
    "            output, ht = self.rnn(x, h0)\n",
    "        elif self.rnn_type is nn.LSTM:\n",
    "            c0 = torch.zeros((self.num_layers * self.directions, batch, self.hidden_size)).to(self.config.device)\n",
    "            output, (ht, ct) = self.rnn(x, (h0, c0))\n",
    "        # decompose layers and directions, (layer, directions, batch, embedding_dim)\n",
    "        ht = ht.view(self.num_layers, self.directions, ht.shape[1], -1) \n",
    "        ht = ht[-1]  # get last layer and remove layer dimension, (directions, batch, embedding_dim)\n",
    "        ht = ht.permute(1, 0, 2)  # (batch, directions, embedding_dim)\n",
    "        ht = ht.contiguous().view(batch, -1)\n",
    "        x = F.relu(self.fc1(ht))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = x.view(batch, -1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[5 seconds](epoch: 0/200)[25000 samples] loss: 0.694.\n"
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "forward() missing 1 required positional argument: 'x_lens'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-1db494d4d561>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;31m# run(rnn_type=nn.GRU, num_layers=1, num_directions=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;31m# run(nn.LSTM, num_layers=3, num_directions=2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_directions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-1db494d4d561>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(rnn_type, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mclassification_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;31m# run(rnn_type=nn.GRU, num_layers=3, num_directions=2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/source/ml_playground/trainer/classification_trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, config, train_dataloader, test_dataloader, check_interval, **kwargs)\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0mtotal_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0meval_word_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_x_lens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_y_lens\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m                     \u001b[0meval_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_word_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m                     \u001b[0meval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriteria\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                     \u001b[0macc_eval_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0meval_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() missing 1 required positional argument: 'x_lens'"
     ]
    }
   ],
   "source": [
    "# put everything together\n",
    "import time\n",
    "from data_util import *\n",
    "from models import *\n",
    "from trainer import classification_trainer\n",
    "from util import config\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def run(rnn_type, **kwargs):\n",
    "    cfg = config.Config(\n",
    "        rnn_type=rnn_type,\n",
    "        criteria=nn.CrossEntropyLoss, optimizer=optim.Adam, lr=0.00002, epochs=200, \n",
    "        batch_size=1024, num_classes=2, sentence_max_length=200, word_embedding_length=128, \n",
    "        activation=F.relu, dropout=0.1, **kwargs\n",
    "    )\n",
    "\n",
    "    pos_train_data_folder = os.path.join(dataset_folder_path, \"train/pos\")\n",
    "    neg_train_data_folder = os.path.join(dataset_folder_path, \"train/neg\")\n",
    "    train_dataset = MovieReviewDataset(cfg, pos_train_data_folder, neg_train_data_folder, word_to_id, \n",
    "                                transform=transforms.Compose([\n",
    "                                    TruncateTransform(cfg), \n",
    "                                    WordsToIdsTransform(cfg, word_to_id),\n",
    "                                ]))\n",
    "\n",
    "    pos_test_data_folder = os.path.join(dataset_folder_path, \"test/pos\")\n",
    "    neg_test_data_folder = os.path.join(dataset_folder_path, \"test/neg\")\n",
    "    test_dataset = MovieReviewDataset(cfg, pos_test_data_folder, neg_test_data_folder, word_to_id, \n",
    "                                transform=transforms.Compose([\n",
    "                                    TruncateTransform(cfg), \n",
    "                                    WordsToIdsTransform(cfg, word_to_id),\n",
    "                                ]))\n",
    "\n",
    "    train_dataloader = DataLoader(dataset=train_dataset, batch_size=cfg.batch_size, shuffle=True, collate_fn=data_util.pad_collate)\n",
    "    test_dataloader = DataLoader(dataset=test_dataset, batch_size=cfg.batch_size, collate_fn=data_util.pad_collate)\n",
    "\n",
    "    # model = TextCNN(cfg, len(word_to_id)).to(device)\n",
    "    model = RNN(cfg, len(word_list)).to(cfg.device)\n",
    "\n",
    "    classification_trainer.train(model=model, config=cfg, train_dataloader=train_dataloader, test_dataloader=test_dataloader, check_interval=10)\n",
    "\n",
    "# run(rnn_type=nn.GRU, num_layers=3, num_directions=2)\n",
    "# run(rnn_type=nn.GRU, num_layers=1, num_directions=1)\n",
    "# run(nn.LSTM, num_layers=3, num_directions=2)\n",
    "run(nn.LSTM, num_layers=1, num_directions=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tiny example\n",
    "import torch\n",
    "\n",
    "a = [torch.Tensor([1,2,3]), torch.Tensor([1,2,3,4,5,6]), torch.Tensor([1,2,3,4,5])]\n",
    "print('shapes of a:', [t.shape for t in a])\n",
    "\n",
    "pad = torch.nn.utils.rnn.pad_sequence(a)\n",
    "print('pad:', pad)\n",
    "\n",
    "packed_pad = torch.nn.utils.rnn.pack_padded_sequence(pad, lengths=[t.shape[0] for t in a], enforce_sorted=False)\n",
    "print('packed_pad:', packed_pad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}